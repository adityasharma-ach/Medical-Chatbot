{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd9bfbe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "304070bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\HP\\\\Desktop\\\\Aditya Sharma\\\\Aditya\\\\Chatbot\\\\Medical-Chatbot\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a60a8c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65fb5d21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\HP\\\\Desktop\\\\Aditya Sharma\\\\Aditya\\\\Chatbot\\\\Medical-Chatbot'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89968636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = 'data1.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a765b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader, Docx2txtLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e9354bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files(data):\n",
    "    loader = DirectoryLoader(data, glob=\"*.pdf\", loader_cls=PyPDFLoader)\n",
    "    document = loader.load()\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bda98358",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_data = load_files(\"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "012ad93a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'data\\\\datascience.pdf', 'total_pages': 32, 'page': 0, 'page_label': '1'}, page_content='with These\\n100 Ques tions\\nData Scientist \\nInterview\\nAce your Next'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'data\\\\datascience.pdf', 'total_pages': 32, 'page': 1, 'page_label': '2'}, page_content=\"C u r a t e d  b y\\nWhat is the role of a data scientist in an \\norganisation?\\nExplain the difference between supervised and \\nunsupervised learning.\\r\\nWhat is cross-validation, and why is it important?\\r\\nA data scientist is responsible for collecting, analysing, \\nand interpreting complex data to help organisations \\nmake informed decisions.\\nSupervised learning uses labelled data for training, \\nwhile unsupervised learning\\rworks with unlabeled \\ndata to find hidden patterns or relationships.\\r\\nCross-validation is a technique used to assess how \\nwell a model generalises to\\ran independent dataset. It \\nis important for evaluating a model's performance and \\npreventing\\roverfitting.\\r\\nQ.1\\nQ.2\\nQ.3\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'data\\\\datascience.pdf', 'total_pages': 32, 'page': 2, 'page_label': '3'}, page_content='C u r a t e d  b y\\nCan you explain the steps involved in the \\ndata preprocessing process?\\r\\nWhat are some common algorithms used in \\nmachine learning?\\r\\nData preprocessing includes data cleaning, handling \\nmissing values, data\\rtransformation, normalisation, \\nand standardisation to prepare the data for analysis \\nand\\rmodelling.\\r\\nCommon machine learning algorithms include linear \\nregression, logistic\\rregression, decision trees, random \\nforests, support vector machines, and neural networks.\\nQ.4\\nQ.5\\nHow do you handle missing data in a dataset?\\nMissing data can be handled by either removing the \\nrows with missing values,\\rimputing the missing values \\nusing statistical techniques, or using advanced \\nimputation\\rmethods such as K-Nearest Neighbors.\\nQ.6'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'data\\\\datascience.pdf', 'total_pages': 32, 'page': 3, 'page_label': '4'}, page_content=\"C u r a t e d  b y\\nWhat is the purpose of the K-Means clustering \\nalgorithm?\\r\\nHow do you assess the performance of a machine \\nlearning model?\\r\\nExplain the term 'bias' in the context of machine \\nlearning models.\\r\\nWhat is the importance of feature scaling in \\nmachine learning?\\nThe K-Means algorithm is used for partitioning a \\ndataset into K clusters, aiming to\\rminimise the sum of \\nsquares within each cluster.\\r\\nModel performance can be assessed using metrics \\nsuch as accuracy, precision,\\rrecall, F1 score, and the \\nROC curve for classification tasks, and metrics such as \\nmean\\rsquared error for regression tasks.\\nBias refers to the error introduced by approximating a \\nreal-world problem, often\\rdue to oversimplification of \\nthe model. High bias can result in underfitting.\\r\\n Feature scaling ensures that the features are at a \\nsimilar scale, preventing\\rcertain features from \\ndominating the learning process and helping the \\nalgorithm converge\\rfaster.\\nQ.7\\nQ.8\\nQ.9\\nQ.10\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'data\\\\datascience.pdf', 'total_pages': 32, 'page': 4, 'page_label': '5'}, page_content='C u r a t e d  b y\\nCan you explain the concept of regularisation in \\nmachine learning?\\r\\nWhat is the difference between L1 and L2 \\nregularisation?\\nRegularisation is a technique used to prevent \\noverfitting by adding a penalty term\\rto the loss \\nfunction, discouraging complex models.\\nL1 regularisation adds the absolute value of the \\nmagnitude of coefficients as a\\rpenalty term, while L2 \\nregularisation adds the square of the magnitude of \\ncoefficients as a\\rpenalty term.\\r\\nQ.11\\nQ.12\\nWhat is the purpose of a confusion matrix in \\nclassification tasks?\\r\\nA confusion matrix is used to visualise the performance \\nof a classification model,\\rshowing the counts of true \\npositive, true negative, false positive, and false \\nnegative\\rpredictions.\\nQ.13'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'data\\\\datascience.pdf', 'total_pages': 32, 'page': 5, 'page_label': '6'}, page_content='C u r a t e d  b y\\nHow do you handle multicollinearity in a dataset?\\nCan you explain the difference between precision \\nand recall?\\nWhat is the purpose of the Naive Bayes algorithm \\nin machine learning?\\nMulticollinearity  can be handled by \\ntechniques such as removing one of \\nthe\\rcorrelated features, using principal \\ncomponent analysis, or using \\nregularisation techniques to\\rreduce \\nthe impact of correlated features.\\nPrecision refers to the ratio of correctly predicted \\npositive observations to the\\rtotal predicted positive \\nobservations, while recall refers to the ratio of correctly \\npredicted\\rpositive observations to the total actual \\npositive observations.\\nThe Naive Bayes algorithm is used for classification \\ntasks, based on the Bayes\\rtheorem with the \\nassumption of independence between features.\\nQ.14\\nQ.15\\nQ.16'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'data\\\\datascience.pdf', 'total_pages': 32, 'page': 6, 'page_label': '7'}, page_content='C u r a t e d  b y\\nHow do you handle outliers in a dataset?\\nExplain the concept of the Central Limit Theorem.\\nWhat is the purpose of a decision tree algorithm in \\nmachine learning?\\nCan you explain the concept of ensemble \\nlearning?\\nOutliers can be handled by either removing them if \\nthey are due to data entry\\rerrors, or by transforming \\nthem using techniques such as winsorization or log \\ntransformation.\\nThe Central Limit Theorem states that the sampling \\ndistribution of the sample\\rmeans approaches a normal \\ndistribution as the sample size increases, regardless of \\nthe\\rshape of the population distribution.\\nDecision trees are used for both classification and \\nregression tasks, creating a\\rmodel that predicts the \\nvalue of a target variable by learning simple decision \\nrules inferred\\rfrom the data features.\\nEnsemble learning involves combining multiple \\nindividual models to improve the\\roverall performance \\nand predictive power of the learning algorithm.\\nQ.17\\nQ.18\\nQ.19\\nQ.20'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'data\\\\datascience.pdf', 'total_pages': 32, 'page': 7, 'page_label': '8'}, page_content='C u r a t e d  b y\\nWhat is the difference between bagging and \\nboosting?\\nExplain the purpose of the Random Forest \\nalgorithm in machine learning.\\nHow do you select the optimal number of clusters \\nin a K-Means clustering algorithm?\\nBagging involves training each model in the ensemble \\nwith a subset of the data,\\rwhile boosting focuses on \\ntraining each model sequentially, giving more weight \\nto the\\rmisclassified data points.\\nRandom Forest is an ensemble learning method that \\nconstructs multiple decision\\rtrees during training and \\noutputs the mode of the classes or the mean \\nprediction of the\\rindividual trees for classification and \\nregression tasks, respectively.\\nThe optimal number of clusters can be determined \\nusing techniques such as the\\relbow method, silhouette \\nscore, or the gap statistic.\\nQ.21\\nQ.22\\nQ.23'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'data\\\\datascience.pdf', 'total_pages': 32, 'page': 8, 'page_label': '9'}, page_content='C u r a t e d  b y\\nWhat is the purpose of the Support Vector Machine \\n(SVM) algorithm?\\nHow do you handle a large volume of data that \\ncannot fit into memory?\\nCan you explain the purpose of a recommendation \\nsystem?\\nWhat is the purpose of Principal Component \\nAnalysis (PCA) in machine learning?\\nSupport Vector Machines are used for classification \\nand regression analysis, with\\rthe primary goal of \\nfinding the hyperplane that best separates the classes.\\nLarge volumes of data can be handled using \\ntechniques such as data streaming,\\rdistributed \\ncomputing frameworks like Hadoop or Spark, and \\ndata compression techniques.\\nRecommendation systems are used to predict and \\nrecommend items or products\\rthat a user may be \\ninterested in, based on their past preferences or \\nbehaviour.\\nPrincipal Component Analysis is used for dimensionality \\nreduction, transforming\\ra large set of variables into a \\nsmaller set of uncorrelated variables while retaining \\nmost of the\\rinformation.\\nQ.24\\nQ.25\\nQ.26\\nQ.27'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'data\\\\datascience.pdf', 'total_pages': 32, 'page': 9, 'page_label': '10'}, page_content='C u r a t e d  b y\\nHow do you handle a situation where the data is \\ntoo imbalanced?\\nWhat is the purpose of a Recurrent Neural Network \\n(RNN) in deep learning?\\nExplain the concept of a Long Short-Term Memory \\n(LSTM) network.\\nImbalanced data can be handled using techniques \\nsuch as oversampling the\\rminority class, \\nundersampling the majority class, or using algorithms \\nspecifically designed to\\rhandle imbalanced datasets.\\nRecurrent Neural Networks are used for sequence data, \\nallowing information to\\rpersist over time, making them \\nsuitable for tasks such as natural language processing \\nand\\rtime series analysis.\\nLSTM networks are a type of RNN that addresses the \\nvanishing gradient\\rproblem, making them more \\neffective for learning and predicting sequences of data.\\nQ.28\\nQ.29\\nQ.30'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'data\\\\datascience.pdf', 'total_pages': 32, 'page': 10, 'page_label': '11'}, page_content='C u r a t e d  b y\\nWhat is the purpose of the Word2Vec algorithm in \\nnatural language processing?\\nHow do you handle a situation where there are \\ntoo many features compared to the\\rnumber of \\nobservations?\\nExplain the concept of a support vector in the \\ncontext of a Support Vector Machine\\ralgorithm.\\nWord2Vec is used for learning word embeddings, \\nrepresenting words as vectors\\rto capture semantic \\nrelationships between words in a text corpus.\\nThe situation of having too many features compared \\nto the number of\\robservations can be handled by \\nusing feature selection techniques, such as Lasso\\r\\nregression, or by using dimensionality reduction \\ntechniques like PCA or t-SNE.\\r\\nSupport vectors are data points that lie closest to the \\ndecision boundary between\\rthe classes, influencing the \\nposition and orientation of the hyperplane in a Support \\nVector\\rMachine.\\nQ.31\\nQ.32\\nQ.33'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'data\\\\datascience.pdf', 'total_pages': 32, 'page': 11, 'page_label': '12'}, page_content='C u r a t e d  b y\\nWhat is the purpose of the Root Mean Square Error \\n(RMSE) metric in regression tasks?\\nCan you explain the purpose of the Apriori \\nalgorithm in association rule mining?\\nHow do you handle a situation where the data is \\nhighly skewed?\\nThe Root Mean Square Error is a commonly used metric \\nfor evaluating the\\raccuracy of a regression model by \\nmeasuring the differences between the predicted \\nvalues\\rand the actual values.\\nThe Apriori algorithm is used for discovering frequent \\nitemsets within a\\rtransactional database and is \\ncommonly employed in market basket analysis to \\nidentify\\rpatterns or relationships between different \\nitems.\\nHighly skewed data can be handled by using \\ntransformations such as log\\rtransformations, square \\nroot transformations, or by using specialised models \\nthat can handle\\rskewed data more effectively.\\nQ.34\\nQ.35\\nQ.36'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'data\\\\datascience.pdf', 'total_pages': 32, 'page': 12, 'page_label': '13'}, page_content='C u r a t e d  b y\\nWhat is the purpose of the Mean Average Precision \\n(MAP) metric in evaluating\\rinformation retrieval \\nsystems?\\nExplain the purpose of the Euclidean distance \\nmetric in clustering tasks.\\nHow do you handle a situation where the data is \\nnot linearly separable?\\nMean Average Precision is used to evaluate the \\nperformance of information\\rretrieval systems, \\nmeasuring the average precision at each relevant \\ndocument retrieved\\racross multiple queries.\\nThe Euclidean distance metric is used to measure the \\ndistance between two\\rpoints in a multidimensional \\nspace and is commonly used in clustering algorithms \\nsuch as\\rK-Means.\\nIn cases where the data is not linearly separable, kernel \\nfunctions can be used in\\ralgorithms like Support Vector \\nMachines to map the data to a higher-dimensional \\nspace\\rwhere it becomes linearly separable.\\nQ.37\\nQ.38\\nQ.39'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'data\\\\datascience.pdf', 'total_pages': 32, 'page': 13, 'page_label': '14'}, page_content='C u r a t e d  b y\\nWhat is the purpose of the Chi-square test in \\nfeature selection?\\nCan you explain the purpose of the Gradient \\nDescent algorithm in machine learning?\\nHow do you handle a situation where the data is \\ntime-series data?\\nThe Chi-square test is used to determine the \\nindependence of two categorical\\rvariables, making it \\nsuitable for feature selection in classification tasks.\\nGradient Descent is an optimization algorithm used to \\nminimise the cost function\\rand find the optimal \\nparameters of a model by iteratively updating the \\nparameters in the\\rdirection of the steepest descent.\\nTime-series data can be handled using techniques \\nsuch as autoregressive\\rintegrated moving average \\n(ARIMA) models, exponential smoothing methods, or \\nmore\\radvanced deep learning models like Long Short-\\nTerm Memory (LSTM) networks.\\nQ.40\\nQ.41\\nQ.42'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'data\\\\datascience.pdf', 'total_pages': 32, 'page': 14, 'page_label': '15'}, page_content='C u r a t e d  b y\\nWhat is the purpose of the K-Nearest Neighbors \\n(KNN) algorithm in machine learning?\\nExplain the purpose of the Log Loss metric in \\nevaluating classification models.\\nHow do you handle a situation where the data is \\nhigh-dimensional?\\nThe K-Nearest Neighbors algorithm is used for \\nclassification and regression\\rtasks, making predictions \\nbased on the majority vote of its k nearest neighbours.\\nLog Loss is used to evaluate the performance of a \\nclassification model that\\routputs probabilities, \\nmeasuring the performance based on the likelihood of \\nthe predicted\\rprobabilities matching the actual labels.\\nHigh-dimensional data can be handled by using \\ndimensionality reduction\\rtechniques such as Principal \\nComponent Analysis (PCA), t-Distributed Stochastic \\nNeighbour\\rEmbedding (t-SNE), or by employing feature \\nselection methods.\\nQ.43\\nQ.44\\nQ.45'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'data\\\\datascience.pdf', 'total_pages': 32, 'page': 15, 'page_label': '16'}, page_content='C u r a t e d  b y\\nWhat is the purpose of the R-squared (R2) metric \\nin evaluating regression models?\\nCan you explain the purpose of the Gini index in \\nthe context of a decision tree\\ralgorithm?\\nHow do you handle a situation where there is noise \\nin the data?\\r\\nR-squared is a statistical measure that represents the \\nproportion of the variance\\rfor a dependent variable \\nthat is explained by an independent variable in a \\nregression model.\\nThe Gini index is used to measure the impurity or the \\nhomogeneity of a node in a\\rdecision tree, helping to \\ndetermine the best split for creating a more accurate \\ndecision tree.\\r\\nNoise in the data can be handled by smoothing \\ntechniques such as moving\\raverages, using robust \\nstatistics, or employing filtering methods to remove \\noutliers and\\rirrelevant data points.\\nQ.46\\nQ.47\\nQ.48'),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'data\\\\datascience.pdf', 'total_pages': 32, 'page': 16, 'page_label': '17'}, page_content=\"C u r a t e d  b y\\nWhat is the purpose of the F1 score metric in \\nevaluating classification models?\\nWhat is the difference between classification and \\nregression in machine learning?\\nCan you explain the bias-variance trade-off in the \\ncontext of model complexity?\\nCan you explain the purpose of the LDA \\n(Linear Discriminant Analysis) algorithm in\\r\\nmachine learning?\\nThe F1 score is the harmonic mean of precision and \\nrecall and is used to\\revaluate the balance between \\nprecision and recall in a classification model.\\nClassification is used to predict discrete categories, \\nwhile regression is used to\\rpredict continuous \\nquantities.\\r\\nThe bias-variance trade-off highlights the trade-off \\nbetween a model's ability to\\rminimise errors due to \\nbias and variance. Increasing model complexity \\nreduces bias but\\rincreases variance and vice versa.\\nLinear Discriminant Analysis is used for dimensionality \\nreduction and\\rclassification tasks, aiming to find the \\nlinear combinations of features that best separate\\r\\nmultiple classes in the data.\\nQ.49\\nQ.51\\nQ.52\\nQ.50\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'data\\\\datascience.pdf', 'total_pages': 32, 'page': 17, 'page_label': '18'}, page_content=\"C u r a t e d  b y\\nHow do you handle imbalanced data sets when \\nbuilding a classification model?\\nExplain the purpose of the term 'regularisation' in \\nmachine learning models.\\nHow do you assess the performance of a \\nclassification model apart from accuracy?\\nWhat is the purpose of the term 'gradient descent' \\nin the context of optimising a model?\\nImbalanced datasets can be handled using techniques \\nlike oversampling,\\rundersampling, or using algorithms \\ndesigned for imbalanced data such as SMOTE\\r\\n(Synthetic Minority Over-sampling Technique).\\nRegularisation is a technique used to prevent \\noverfitting by adding a penalty term\\rto the loss \\nfunction, discouraging overly complex models.\\nThe performance of a classification model can be \\nevaluated using metrics such as\\rprecision, recall, F1 \\nscore, and the area under the ROC curve.\\nGradient descent is an iterative optimization algorithm \\nused to minimise the cost\\rfunction of a model by \\nadjusting the model's parameters in the direction of \\nsteepest descent.\\r\\nQ.53\\nQ.54\\nQ.56\\nQ.55\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'data\\\\datascience.pdf', 'total_pages': 32, 'page': 18, 'page_label': '19'}, page_content=\"C u r a t e d  b y\\nCan you explain the concept of 'feature selection' \\nand its importance in model building?\\nWhat is the purpose of the term 'cross-validation' \\nin model training and evaluation?\\nHow do you handle missing data in a dataset while \\nbuilding a predictive model?\\nFeature selection involves selecting the most relevant \\nfeatures from a dataset. It is\\rcrucial for improving \\nmodel performance, reducing overfitting, and \\nenhancing interpretability.\\nCross-validation is used to assess how well a model \\ngeneralises to an\\rindependent dataset, minimising the \\nrisk of overfitting and providing a more accurate\\r\\nestimate of the model's performance.\\nMissing data can be handled by \\ntechniques such as mean/median \\nimputation,\\rmode imputation, or \\nusing advanced methods like \\nmultiple imputation or K-Nearest\\r\\nNeighbors imputation.\\nQ.57\\nQ.58\\nQ.59\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'data\\\\datascience.pdf', 'total_pages': 32, 'page': 19, 'page_label': '20'}, page_content=\"C u r a t e d  b y\\nExplain the purpose of the term 'ensemble learning' \\nand its benefits in model building.\\nWhat is the difference between unsupervised and \\nsupervised machine learning\\ralgorithms?\\nCan you explain the concept of 'clustering' and \\nprovide an example of when it is used?\\nWhat is the purpose of 'dimensionality reduction' in \\ndata analysis, and how is it\\rachieved?\\nEnsemble learning involves combining multiple models \\nto improve predictive\\rperformance and reduce \\noverfitting, often resulting in better generalisation and \\nmore robust\\rpredictions.\\r\\nSupervised learning uses labelled data for training, \\nwhile unsupervised learning\\rworks with unlabeled data \\nto find patterns and relationships.\\nClustering is an unsupervised learning technique used \\nto group similar data points\\rtogether. An example is \\ncustomer segmentation in marketing.\\nDimensionality reduction is used to reduce the number \\nof features in a dataset. It is\\rachieved through \\ntechniques like principal component analysis (PCA) \\nand t-distributed\\rstochastic neighbour embedding (t-\\nSNE).\\r\\nQ.60\\nQ.61\\nQ.62\\nQ.63\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'data\\\\datascience.pdf', 'total_pages': 32, 'page': 20, 'page_label': '21'}, page_content=\"C u r a t e d  b y\\nHow do you handle the problem of overfitting in \\nmachine learning models?\\nHow do you handle the problem of multicollinearity \\nin a dataset?\\nExplain the purpose of the term 'Naive Bayes' in \\nmachine learning and its application.\\nWhat is the purpose of the term 'decision trees' in \\nmachine learning, and how does it\\rwork?\\nOverfitting can be mitigated by using techniques like \\ncross-validation,\\rregularisation, early stopping, and \\nreducing model complexity.\\nMulticollinearity can be addressed by techniques such \\nas removing one of the\\rcorrelated features, using \\nprincipal component analysis (PCA), or using \\nregularisation\\rmethods.\\nNaive Bayes is a probabilistic classification algorithm \\nbased on Bayes' theorem\\rwith an assumption of \\nindependence between features. It is commonly used \\nfor text\\rclassification and spam filtering.\\nDecision trees are predictive models that map features \\nto conclusions about the\\rtarget value. They work by \\nsplitting the dataset into smaller subsets based on the \\nmost\\rsignificant differentiators in the data.\\nQ.64\\nQ.67\\nQ.65\\nQ.66\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'data\\\\datascience.pdf', 'total_pages': 32, 'page': 21, 'page_label': '22'}, page_content=\"C u r a t e d  b y\\nCan you explain the purpose of the term 'random \\nforest' in machine learning and its\\radvantages?\\nWhat is the purpose of 'data preprocessing' in \\nmachine learning, and what are some\\rcommon \\ntechniques used?\\nHow do you handle the problem of underfitting in a \\nmachine learning model?\\nRandom forests are an ensemble learning method that \\nconstructs multiple decision\\rtrees during training. They \\nare effective for reducing overfitting and handling large \\ndatasets\\rwith high dimensionality.\\nData preprocessing involves preparing and cleaning \\ndata before it is fed into a\\rmachine learning model. \\nCommon techniques include data normalisation, \\nstandardisation,\\rand handling missing values.\\nUnderfitting can be addressed by using more complex \\nmodels, adding more\\rfeatures, or reducing \\nregularisation, allowing the model to capture more \\ncomplex patterns in\\rthe data.\\nQ.68\\nQ.69\\nQ.70\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'data\\\\datascience.pdf', 'total_pages': 32, 'page': 22, 'page_label': '23'}, page_content=\"C u r a t e d  b y\\nExplain the concept of 'hyperparameter tuning' in \\nmachine learning algorithms.\\nWhat is the purpose of 'ANOVA' (Analysis of \\nVariance) in statistical analysis, and when is\\rit used?\\nHow do you handle a situation where the data has \\noutliers?\\nExplain the concept of 'bias' in machine learning \\nmodels.\\nHyperparameter tuning involves finding the best set of \\nhyperparameters for a\\rmachine learning model to \\noptimise its performance and generalisation.\\nANOVA is used to analyse the differences among group \\nmeans and is applied\\rwhen comparing means of more \\nthan two groups to determine whether they are \\nstatistically\\rsignificantly different.\\nOutliers can be handled by removing them if they are \\ndue to data entry errors or by\\rtransforming them using \\ntechniques such as winsorization or log transformation.\\nBias refers to the error introduced by approximating a \\nreal-world problem, often\\rdue to oversimplification of \\nthe model. High bias can lead to underfitting.\\nQ.71\\nQ.72\\nQ.73\\nQ.74\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'data\\\\datascience.pdf', 'total_pages': 32, 'page': 23, 'page_label': '24'}, page_content=\"C u r a t e d  b y\\nWhat is the purpose of the 'mean squared error' \\nmetric in regression analysis?\\nCan you explain the purpose of the term 'cosine \\nsimilarity' in similarity measurements?\\nHow do you handle a situation where the data has \\na time component?\\nMean squared error is a commonly used metric for \\nevaluating the performance of a\\rregression model by \\nmeasuring the average of the squares of the \\ndifferences between\\rpredicted and actual values.\\nCosine similarity is a metric used to measure the \\nsimilarity between two non-zero\\rvectors, often used in \\ntext mining and collaborative filtering.\\nData with a time component can be analysed using \\ntime series analysis techniques\\rsuch as\\rautoregressive \\nintegrated moving average (ARIMA) models, \\nexponential smoothing, or\\rProphet forecasting models.\\r\\nQ.75\\nQ.76\\nQ.77\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'data\\\\datascience.pdf', 'total_pages': 32, 'page': 24, 'page_label': '25'}, page_content=\"C u r a t e d  b y\\nExplain the concept of 'precision' and 'recall' in the \\ncontext of classification models.\\nWhat is the purpose of the 'Hadoop' framework in \\nbig data processing, and how is it\\rused?\\nHow do you handle a situation where the data has \\na lot of noise?\\nPrecision measures the proportion of true positive \\nresults among the predicted\\rpositive results, while \\nrecall measures the proportion of true positive results \\namong the\\ractual positive results.\\nHadoop is an open-source framework used for \\ndistributed storage and processing\\rof large data sets \\nacross clusters of computers using simple \\nprogramming models.\\nNoisy data can be managed through techniques such \\nas data smoothing, filtering,\\ror by using robust \\nstatistical measures that are less sensitive to outliers.\\nQ.78\\nQ.79\\nQ.80\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'data\\\\datascience.pdf', 'total_pages': 32, 'page': 25, 'page_label': '26'}, page_content=\"C u r a t e d  b y\\nExplain the concept of 'correlation' in statistics and \\nits different types.\\nWhat is the purpose of the 'k-nearest neighbours' \\nalgorithm in machine learning, and\\rhow does it \\nwork?\\nHow do you handle a situation where the data has \\na lot of categorical variables?\\nCorrelation measures the relationship between two \\nvariables and can be positive,\\rnegative, or zero, \\nindicating the strength and direction of the \\nrelationship.\\r\\nThe k-nearest neighbours algorithm is used for \\nclassification and regression tasks,\\rmaking predictions \\nbased on the majority vote or averaging the values of \\nthe k nearest\\rneighbours.\\nCategorical variables can be handled through \\ntechniques such as one-hot\\rencoding, label encoding, \\nor using target encoding to convert them into a format \\nsuitable for\\rmachine learning models.\\nQ.81\\nQ.82\\nQ.83\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'data\\\\datascience.pdf', 'total_pages': 32, 'page': 26, 'page_label': '27'}, page_content=\"t u t o r t  a c a d e m yC u r a t e d  b y\\nExplain the purpose of the 'SVM' (Support Vector \\nMachine) algorithm in machine\\rlearning, and its \\nadvantages.\\nSupport Vector Machines are supervised learning \\nmodels used for classification\\rand regression analysis. \\nThey are effective in high-dimensional spaces and \\nwork well with\\rcomplex datasets.\\nQ.84\\nWhat is the purpose of the 'LSTM' \\n(Long Short-Term Memory) network in deep \\nlearning,\\rand how is it used?\\nCan you explain the purpose of the term 'Principal \\nComponent Analysis' (PCA)\\rin dimensionality \\nreduction, and how is it used?\\nLSTM networks are a type of recurrent neural network \\n(RNN) used for processing\\rand making predictions \\nbased on sequential data, often used in natural \\nlanguage processing\\rand time series analysis.\\nPrincipal Component Analysis is a technique used to \\nreduce the dimensionality of a\\rdataset while \\npreserving as much variance as possible. It transforms \\nthe original variables\\rinto a new set of variables, the \\nprincipal components, which are orthogonal and\\r\\nuncorrelated. This aids in simplifying the dataset and \\nspeeding up the subsequent learning\\ralgorithms while \\nretaining most of the essential information.\\nQ.85\\nQ.86\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'data\\\\datascience.pdf', 'total_pages': 32, 'page': 27, 'page_label': '28'}, page_content=\"C u r a t e d  b y\\nExplain the concept of 'k-means clustering' and its \\napplication in unsupervised learning.\\nWhat is the purpose of the 'R-squared' metric in \\nregression analysis, and what does it\\rindicate \\nabout the model's fit?\\nWhat is the purpose of the term 't-Distributed \\nStochastic Neighbour Embedding' (t-SNE)\\r\\nin dimensionality reduction, and how is it used?\\nK-means clustering is a popular unsupervised learning \\nalgorithm used for\\rpartitioning a dataset into K clusters \\nbased on similarities in the data points.\\nR-squared is a statistical measure that represents the \\nproportion of the variance for\\ra dependent variable \\nexplained by the independent variables in a regression \\nmodel. It\\rindicates the goodness of fit of the model.\\nt-Distributed Stochastic Neighbour Embedding is a \\nnonlinear dimensionality\\rreduction technique used for \\nvisualising high-dimensional data in a low-\\ndimensional space. It\\ris particularly useful for \\nvisualising complex datasets and identifying patterns \\nor clusters\\rwithin the data.\\nQ.87\\nQ.88\\nQ.89\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'data\\\\datascience.pdf', 'total_pages': 32, 'page': 28, 'page_label': '29'}, page_content=\"C u r a t e d  b y\\nExplain the purpose of the 'F1 score' metric in \\nevaluating classification models and its\\r\\nrelationship with precision and recall.\\nCan you explain the concept of 'backpropagation' \\nin neural networks and its role in\\rtraining the \\nmodel?\\nThe F1 score is the harmonic mean of precision and \\nrecall and is used to evaluate\\rthe balance between \\nprecision and recall in a classification model.\\r\\nBackpropagation is an algorithm used to train artificial \\nneural networks by adjusting\\rthe weights of the \\nconnections in the network to minimise the difference \\nbetween predicted\\rand actual outputs.\\nQ.90\\nQ.91\\nWhat is the purpose of the 'chi-square test' in \\nstatistics, and when is it used?\\nThe chi-square test is used to determine the \\nindependence of two categorical\\rvariables and is often \\nused to test the significance of relationships between \\nvariables in a\\rcontingency table.\\r\\nQ.92\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'data\\\\datascience.pdf', 'total_pages': 32, 'page': 29, 'page_label': '30'}, page_content=\"C u r a t e d  b y\\nHow do you handle a situation where the data is \\nnot normally distributed?\\nExplain the concept of 'latent variables' in the \\ncontext of factor analysis and its\\rimportance.\\nWhat is the purpose of the 'Gini index' in decision \\ntrees, and how is it used in the context\\rof building \\nthe tree?\\nNon-normally distributed data can be transformed \\nusing techniques such as the\\rBox-Cox transformation, \\nYeo-Johnson transformation, or log transformation to \\napproximate a\\rnormal distribution.\\nLatent variables are variables that are not directly \\nobserved but are inferred from\\robserved variables. \\nThey are crucial for capturing underlying factors and \\nreducing the\\rdimensionality of the data.\\nThe Gini index is a metric used to measure the impurity \\nof a node in a decision\\rtree. It is used to find the best \\nsplit for creating a more accurate decision tree.\\nQ.93\\nQ.94\\nQ.95\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'data\\\\datascience.pdf', 'total_pages': 32, 'page': 30, 'page_label': '31'}, page_content=\"t u t o r t  a c a d e m yC u r a t e d  b y\\nHow do you handle a situation where the data has \\na lot of continuous variables?\\nExplain the purpose of 'association rules' in data \\nmining, and provide an example of its\\rapplication.\\nWhat is the purpose of the 'logistic function' in \\nlogistic regression, and how is it used for\\r\\nbinary classification?\\nContinuous variables can be handled through \\ntechniques such as scaling and\\rnormalisation to \\nensure that the variables are on a similar scale, \\npreventing certain features\\rfrom dominating the \\nlearning process.\\nAssociation rules are used to discover interesting \\nrelationships between variables\\rin large datasets. An \\nexample is market basket analysis used to identify \\nproducts frequently\\rpurchased together.\\nThe logistic function is used to model the probability of \\na binary outcome. It maps\\rany real-valued number to \\na value between 0 and 1, making it suitable for binary\\r\\nclassification tasks.\\nQ.96\\nQ.97\\nQ.98\"),\n",
       " Document(metadata={'producer': 'PyPDF', 'creator': 'PyPDF', 'creationdate': '', 'source': 'data\\\\datascience.pdf', 'total_pages': 32, 'page': 31, 'page_label': '32'}, page_content=\"C u r a t e d  b y\\nHow do you handle a situation where the data has \\na lot of missing values?\\nExplain the concept of 'bagging' and 'boosting' in \\nensemble learning, and provide an\\r\\nexample of when each technique is used.\\nData with missing values can be managed through \\ntechniques such as imputation,\\rusing algorithms like \\nK-Nearest Neighbours, decision trees, or employing \\nadvanced\\rtechniques like deep learning-based \\nimputation.\\nBagging involves training multiple models \\nindependently and combining their\\rpredictions, while \\nboosting trains models sequentially, giving more \\nweight to misclassified\\rdata points. Bagging is used for \\nreducing variance, while boosting is used for reducing \\nbias\\rin ensemble models.\\nQ.99\\nQ.100\\nHighest \\nCTC\\nHiring\\nPartners350+Career \\nTransitions1250+ 2.1CR\\nWhy Tutort Academy?\")]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6f52859f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4602cf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfd7e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_to_minimal_docs(docs: List[Document]) -> List[Document]:\n",
    "    minimal_docs: List[Document] = []\n",
    "    for doc in docs:\n",
    "        src = doc.metadata.get(\"source\")\n",
    "        minimal_docs.append(Document(page_content=doc.page_content, metadata={\"source\": src}))\n",
    "    return minimal_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e5403e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_docs = filter_to_minimal_docs(extracted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b00ce71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='with These\\n100 Ques tions\\nData Scientist \\nInterview\\nAce your Next'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content=\"C u r a t e d  b y\\nWhat is the role of a data scientist in an \\norganisation?\\nExplain the difference between supervised and \\nunsupervised learning.\\r\\nWhat is cross-validation, and why is it important?\\r\\nA data scientist is responsible for collecting, analysing, \\nand interpreting complex data to help organisations \\nmake informed decisions.\\nSupervised learning uses labelled data for training, \\nwhile unsupervised learning\\rworks with unlabeled \\ndata to find hidden patterns or relationships.\\r\\nCross-validation is a technique used to assess how \\nwell a model generalises to\\ran independent dataset. It \\nis important for evaluating a model's performance and \\npreventing\\roverfitting.\\r\\nQ.1\\nQ.2\\nQ.3\"),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='C u r a t e d  b y\\nCan you explain the steps involved in the \\ndata preprocessing process?\\r\\nWhat are some common algorithms used in \\nmachine learning?\\r\\nData preprocessing includes data cleaning, handling \\nmissing values, data\\rtransformation, normalisation, \\nand standardisation to prepare the data for analysis \\nand\\rmodelling.\\r\\nCommon machine learning algorithms include linear \\nregression, logistic\\rregression, decision trees, random \\nforests, support vector machines, and neural networks.\\nQ.4\\nQ.5\\nHow do you handle missing data in a dataset?\\nMissing data can be handled by either removing the \\nrows with missing values,\\rimputing the missing values \\nusing statistical techniques, or using advanced \\nimputation\\rmethods such as K-Nearest Neighbors.\\nQ.6'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content=\"C u r a t e d  b y\\nWhat is the purpose of the K-Means clustering \\nalgorithm?\\r\\nHow do you assess the performance of a machine \\nlearning model?\\r\\nExplain the term 'bias' in the context of machine \\nlearning models.\\r\\nWhat is the importance of feature scaling in \\nmachine learning?\\nThe K-Means algorithm is used for partitioning a \\ndataset into K clusters, aiming to\\rminimise the sum of \\nsquares within each cluster.\\r\\nModel performance can be assessed using metrics \\nsuch as accuracy, precision,\\rrecall, F1 score, and the \\nROC curve for classification tasks, and metrics such as \\nmean\\rsquared error for regression tasks.\\nBias refers to the error introduced by approximating a \\nreal-world problem, often\\rdue to oversimplification of \\nthe model. High bias can result in underfitting.\\r\\n Feature scaling ensures that the features are at a \\nsimilar scale, preventing\\rcertain features from \\ndominating the learning process and helping the \\nalgorithm converge\\rfaster.\\nQ.7\\nQ.8\\nQ.9\\nQ.10\"),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='C u r a t e d  b y\\nCan you explain the concept of regularisation in \\nmachine learning?\\r\\nWhat is the difference between L1 and L2 \\nregularisation?\\nRegularisation is a technique used to prevent \\noverfitting by adding a penalty term\\rto the loss \\nfunction, discouraging complex models.\\nL1 regularisation adds the absolute value of the \\nmagnitude of coefficients as a\\rpenalty term, while L2 \\nregularisation adds the square of the magnitude of \\ncoefficients as a\\rpenalty term.\\r\\nQ.11\\nQ.12\\nWhat is the purpose of a confusion matrix in \\nclassification tasks?\\r\\nA confusion matrix is used to visualise the performance \\nof a classification model,\\rshowing the counts of true \\npositive, true negative, false positive, and false \\nnegative\\rpredictions.\\nQ.13'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='C u r a t e d  b y\\nHow do you handle multicollinearity in a dataset?\\nCan you explain the difference between precision \\nand recall?\\nWhat is the purpose of the Naive Bayes algorithm \\nin machine learning?\\nMulticollinearity  can be handled by \\ntechniques such as removing one of \\nthe\\rcorrelated features, using principal \\ncomponent analysis, or using \\nregularisation techniques to\\rreduce \\nthe impact of correlated features.\\nPrecision refers to the ratio of correctly predicted \\npositive observations to the\\rtotal predicted positive \\nobservations, while recall refers to the ratio of correctly \\npredicted\\rpositive observations to the total actual \\npositive observations.\\nThe Naive Bayes algorithm is used for classification \\ntasks, based on the Bayes\\rtheorem with the \\nassumption of independence between features.\\nQ.14\\nQ.15\\nQ.16'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='C u r a t e d  b y\\nHow do you handle outliers in a dataset?\\nExplain the concept of the Central Limit Theorem.\\nWhat is the purpose of a decision tree algorithm in \\nmachine learning?\\nCan you explain the concept of ensemble \\nlearning?\\nOutliers can be handled by either removing them if \\nthey are due to data entry\\rerrors, or by transforming \\nthem using techniques such as winsorization or log \\ntransformation.\\nThe Central Limit Theorem states that the sampling \\ndistribution of the sample\\rmeans approaches a normal \\ndistribution as the sample size increases, regardless of \\nthe\\rshape of the population distribution.\\nDecision trees are used for both classification and \\nregression tasks, creating a\\rmodel that predicts the \\nvalue of a target variable by learning simple decision \\nrules inferred\\rfrom the data features.\\nEnsemble learning involves combining multiple \\nindividual models to improve the\\roverall performance \\nand predictive power of the learning algorithm.\\nQ.17\\nQ.18\\nQ.19\\nQ.20'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='C u r a t e d  b y\\nWhat is the difference between bagging and \\nboosting?\\nExplain the purpose of the Random Forest \\nalgorithm in machine learning.\\nHow do you select the optimal number of clusters \\nin a K-Means clustering algorithm?\\nBagging involves training each model in the ensemble \\nwith a subset of the data,\\rwhile boosting focuses on \\ntraining each model sequentially, giving more weight \\nto the\\rmisclassified data points.\\nRandom Forest is an ensemble learning method that \\nconstructs multiple decision\\rtrees during training and \\noutputs the mode of the classes or the mean \\nprediction of the\\rindividual trees for classification and \\nregression tasks, respectively.\\nThe optimal number of clusters can be determined \\nusing techniques such as the\\relbow method, silhouette \\nscore, or the gap statistic.\\nQ.21\\nQ.22\\nQ.23'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='C u r a t e d  b y\\nWhat is the purpose of the Support Vector Machine \\n(SVM) algorithm?\\nHow do you handle a large volume of data that \\ncannot fit into memory?\\nCan you explain the purpose of a recommendation \\nsystem?\\nWhat is the purpose of Principal Component \\nAnalysis (PCA) in machine learning?\\nSupport Vector Machines are used for classification \\nand regression analysis, with\\rthe primary goal of \\nfinding the hyperplane that best separates the classes.\\nLarge volumes of data can be handled using \\ntechniques such as data streaming,\\rdistributed \\ncomputing frameworks like Hadoop or Spark, and \\ndata compression techniques.\\nRecommendation systems are used to predict and \\nrecommend items or products\\rthat a user may be \\ninterested in, based on their past preferences or \\nbehaviour.\\nPrincipal Component Analysis is used for dimensionality \\nreduction, transforming\\ra large set of variables into a \\nsmaller set of uncorrelated variables while retaining \\nmost of the\\rinformation.\\nQ.24\\nQ.25\\nQ.26\\nQ.27'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='C u r a t e d  b y\\nHow do you handle a situation where the data is \\ntoo imbalanced?\\nWhat is the purpose of a Recurrent Neural Network \\n(RNN) in deep learning?\\nExplain the concept of a Long Short-Term Memory \\n(LSTM) network.\\nImbalanced data can be handled using techniques \\nsuch as oversampling the\\rminority class, \\nundersampling the majority class, or using algorithms \\nspecifically designed to\\rhandle imbalanced datasets.\\nRecurrent Neural Networks are used for sequence data, \\nallowing information to\\rpersist over time, making them \\nsuitable for tasks such as natural language processing \\nand\\rtime series analysis.\\nLSTM networks are a type of RNN that addresses the \\nvanishing gradient\\rproblem, making them more \\neffective for learning and predicting sequences of data.\\nQ.28\\nQ.29\\nQ.30'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='C u r a t e d  b y\\nWhat is the purpose of the Word2Vec algorithm in \\nnatural language processing?\\nHow do you handle a situation where there are \\ntoo many features compared to the\\rnumber of \\nobservations?\\nExplain the concept of a support vector in the \\ncontext of a Support Vector Machine\\ralgorithm.\\nWord2Vec is used for learning word embeddings, \\nrepresenting words as vectors\\rto capture semantic \\nrelationships between words in a text corpus.\\nThe situation of having too many features compared \\nto the number of\\robservations can be handled by \\nusing feature selection techniques, such as Lasso\\r\\nregression, or by using dimensionality reduction \\ntechniques like PCA or t-SNE.\\r\\nSupport vectors are data points that lie closest to the \\ndecision boundary between\\rthe classes, influencing the \\nposition and orientation of the hyperplane in a Support \\nVector\\rMachine.\\nQ.31\\nQ.32\\nQ.33'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='C u r a t e d  b y\\nWhat is the purpose of the Root Mean Square Error \\n(RMSE) metric in regression tasks?\\nCan you explain the purpose of the Apriori \\nalgorithm in association rule mining?\\nHow do you handle a situation where the data is \\nhighly skewed?\\nThe Root Mean Square Error is a commonly used metric \\nfor evaluating the\\raccuracy of a regression model by \\nmeasuring the differences between the predicted \\nvalues\\rand the actual values.\\nThe Apriori algorithm is used for discovering frequent \\nitemsets within a\\rtransactional database and is \\ncommonly employed in market basket analysis to \\nidentify\\rpatterns or relationships between different \\nitems.\\nHighly skewed data can be handled by using \\ntransformations such as log\\rtransformations, square \\nroot transformations, or by using specialised models \\nthat can handle\\rskewed data more effectively.\\nQ.34\\nQ.35\\nQ.36'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='C u r a t e d  b y\\nWhat is the purpose of the Mean Average Precision \\n(MAP) metric in evaluating\\rinformation retrieval \\nsystems?\\nExplain the purpose of the Euclidean distance \\nmetric in clustering tasks.\\nHow do you handle a situation where the data is \\nnot linearly separable?\\nMean Average Precision is used to evaluate the \\nperformance of information\\rretrieval systems, \\nmeasuring the average precision at each relevant \\ndocument retrieved\\racross multiple queries.\\nThe Euclidean distance metric is used to measure the \\ndistance between two\\rpoints in a multidimensional \\nspace and is commonly used in clustering algorithms \\nsuch as\\rK-Means.\\nIn cases where the data is not linearly separable, kernel \\nfunctions can be used in\\ralgorithms like Support Vector \\nMachines to map the data to a higher-dimensional \\nspace\\rwhere it becomes linearly separable.\\nQ.37\\nQ.38\\nQ.39'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='C u r a t e d  b y\\nWhat is the purpose of the Chi-square test in \\nfeature selection?\\nCan you explain the purpose of the Gradient \\nDescent algorithm in machine learning?\\nHow do you handle a situation where the data is \\ntime-series data?\\nThe Chi-square test is used to determine the \\nindependence of two categorical\\rvariables, making it \\nsuitable for feature selection in classification tasks.\\nGradient Descent is an optimization algorithm used to \\nminimise the cost function\\rand find the optimal \\nparameters of a model by iteratively updating the \\nparameters in the\\rdirection of the steepest descent.\\nTime-series data can be handled using techniques \\nsuch as autoregressive\\rintegrated moving average \\n(ARIMA) models, exponential smoothing methods, or \\nmore\\radvanced deep learning models like Long Short-\\nTerm Memory (LSTM) networks.\\nQ.40\\nQ.41\\nQ.42'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='C u r a t e d  b y\\nWhat is the purpose of the K-Nearest Neighbors \\n(KNN) algorithm in machine learning?\\nExplain the purpose of the Log Loss metric in \\nevaluating classification models.\\nHow do you handle a situation where the data is \\nhigh-dimensional?\\nThe K-Nearest Neighbors algorithm is used for \\nclassification and regression\\rtasks, making predictions \\nbased on the majority vote of its k nearest neighbours.\\nLog Loss is used to evaluate the performance of a \\nclassification model that\\routputs probabilities, \\nmeasuring the performance based on the likelihood of \\nthe predicted\\rprobabilities matching the actual labels.\\nHigh-dimensional data can be handled by using \\ndimensionality reduction\\rtechniques such as Principal \\nComponent Analysis (PCA), t-Distributed Stochastic \\nNeighbour\\rEmbedding (t-SNE), or by employing feature \\nselection methods.\\nQ.43\\nQ.44\\nQ.45'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='C u r a t e d  b y\\nWhat is the purpose of the R-squared (R2) metric \\nin evaluating regression models?\\nCan you explain the purpose of the Gini index in \\nthe context of a decision tree\\ralgorithm?\\nHow do you handle a situation where there is noise \\nin the data?\\r\\nR-squared is a statistical measure that represents the \\nproportion of the variance\\rfor a dependent variable \\nthat is explained by an independent variable in a \\nregression model.\\nThe Gini index is used to measure the impurity or the \\nhomogeneity of a node in a\\rdecision tree, helping to \\ndetermine the best split for creating a more accurate \\ndecision tree.\\r\\nNoise in the data can be handled by smoothing \\ntechniques such as moving\\raverages, using robust \\nstatistics, or employing filtering methods to remove \\noutliers and\\rirrelevant data points.\\nQ.46\\nQ.47\\nQ.48'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content=\"C u r a t e d  b y\\nWhat is the purpose of the F1 score metric in \\nevaluating classification models?\\nWhat is the difference between classification and \\nregression in machine learning?\\nCan you explain the bias-variance trade-off in the \\ncontext of model complexity?\\nCan you explain the purpose of the LDA \\n(Linear Discriminant Analysis) algorithm in\\r\\nmachine learning?\\nThe F1 score is the harmonic mean of precision and \\nrecall and is used to\\revaluate the balance between \\nprecision and recall in a classification model.\\nClassification is used to predict discrete categories, \\nwhile regression is used to\\rpredict continuous \\nquantities.\\r\\nThe bias-variance trade-off highlights the trade-off \\nbetween a model's ability to\\rminimise errors due to \\nbias and variance. Increasing model complexity \\nreduces bias but\\rincreases variance and vice versa.\\nLinear Discriminant Analysis is used for dimensionality \\nreduction and\\rclassification tasks, aiming to find the \\nlinear combinations of features that best separate\\r\\nmultiple classes in the data.\\nQ.49\\nQ.51\\nQ.52\\nQ.50\"),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content=\"C u r a t e d  b y\\nHow do you handle imbalanced data sets when \\nbuilding a classification model?\\nExplain the purpose of the term 'regularisation' in \\nmachine learning models.\\nHow do you assess the performance of a \\nclassification model apart from accuracy?\\nWhat is the purpose of the term 'gradient descent' \\nin the context of optimising a model?\\nImbalanced datasets can be handled using techniques \\nlike oversampling,\\rundersampling, or using algorithms \\ndesigned for imbalanced data such as SMOTE\\r\\n(Synthetic Minority Over-sampling Technique).\\nRegularisation is a technique used to prevent \\noverfitting by adding a penalty term\\rto the loss \\nfunction, discouraging overly complex models.\\nThe performance of a classification model can be \\nevaluated using metrics such as\\rprecision, recall, F1 \\nscore, and the area under the ROC curve.\\nGradient descent is an iterative optimization algorithm \\nused to minimise the cost\\rfunction of a model by \\nadjusting the model's parameters in the direction of \\nsteepest descent.\\r\\nQ.53\\nQ.54\\nQ.56\\nQ.55\"),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content=\"C u r a t e d  b y\\nCan you explain the concept of 'feature selection' \\nand its importance in model building?\\nWhat is the purpose of the term 'cross-validation' \\nin model training and evaluation?\\nHow do you handle missing data in a dataset while \\nbuilding a predictive model?\\nFeature selection involves selecting the most relevant \\nfeatures from a dataset. It is\\rcrucial for improving \\nmodel performance, reducing overfitting, and \\nenhancing interpretability.\\nCross-validation is used to assess how well a model \\ngeneralises to an\\rindependent dataset, minimising the \\nrisk of overfitting and providing a more accurate\\r\\nestimate of the model's performance.\\nMissing data can be handled by \\ntechniques such as mean/median \\nimputation,\\rmode imputation, or \\nusing advanced methods like \\nmultiple imputation or K-Nearest\\r\\nNeighbors imputation.\\nQ.57\\nQ.58\\nQ.59\"),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content=\"C u r a t e d  b y\\nExplain the purpose of the term 'ensemble learning' \\nand its benefits in model building.\\nWhat is the difference between unsupervised and \\nsupervised machine learning\\ralgorithms?\\nCan you explain the concept of 'clustering' and \\nprovide an example of when it is used?\\nWhat is the purpose of 'dimensionality reduction' in \\ndata analysis, and how is it\\rachieved?\\nEnsemble learning involves combining multiple models \\nto improve predictive\\rperformance and reduce \\noverfitting, often resulting in better generalisation and \\nmore robust\\rpredictions.\\r\\nSupervised learning uses labelled data for training, \\nwhile unsupervised learning\\rworks with unlabeled data \\nto find patterns and relationships.\\nClustering is an unsupervised learning technique used \\nto group similar data points\\rtogether. An example is \\ncustomer segmentation in marketing.\\nDimensionality reduction is used to reduce the number \\nof features in a dataset. It is\\rachieved through \\ntechniques like principal component analysis (PCA) \\nand t-distributed\\rstochastic neighbour embedding (t-\\nSNE).\\r\\nQ.60\\nQ.61\\nQ.62\\nQ.63\"),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content=\"C u r a t e d  b y\\nHow do you handle the problem of overfitting in \\nmachine learning models?\\nHow do you handle the problem of multicollinearity \\nin a dataset?\\nExplain the purpose of the term 'Naive Bayes' in \\nmachine learning and its application.\\nWhat is the purpose of the term 'decision trees' in \\nmachine learning, and how does it\\rwork?\\nOverfitting can be mitigated by using techniques like \\ncross-validation,\\rregularisation, early stopping, and \\nreducing model complexity.\\nMulticollinearity can be addressed by techniques such \\nas removing one of the\\rcorrelated features, using \\nprincipal component analysis (PCA), or using \\nregularisation\\rmethods.\\nNaive Bayes is a probabilistic classification algorithm \\nbased on Bayes' theorem\\rwith an assumption of \\nindependence between features. It is commonly used \\nfor text\\rclassification and spam filtering.\\nDecision trees are predictive models that map features \\nto conclusions about the\\rtarget value. They work by \\nsplitting the dataset into smaller subsets based on the \\nmost\\rsignificant differentiators in the data.\\nQ.64\\nQ.67\\nQ.65\\nQ.66\"),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content=\"C u r a t e d  b y\\nCan you explain the purpose of the term 'random \\nforest' in machine learning and its\\radvantages?\\nWhat is the purpose of 'data preprocessing' in \\nmachine learning, and what are some\\rcommon \\ntechniques used?\\nHow do you handle the problem of underfitting in a \\nmachine learning model?\\nRandom forests are an ensemble learning method that \\nconstructs multiple decision\\rtrees during training. They \\nare effective for reducing overfitting and handling large \\ndatasets\\rwith high dimensionality.\\nData preprocessing involves preparing and cleaning \\ndata before it is fed into a\\rmachine learning model. \\nCommon techniques include data normalisation, \\nstandardisation,\\rand handling missing values.\\nUnderfitting can be addressed by using more complex \\nmodels, adding more\\rfeatures, or reducing \\nregularisation, allowing the model to capture more \\ncomplex patterns in\\rthe data.\\nQ.68\\nQ.69\\nQ.70\"),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content=\"C u r a t e d  b y\\nExplain the concept of 'hyperparameter tuning' in \\nmachine learning algorithms.\\nWhat is the purpose of 'ANOVA' (Analysis of \\nVariance) in statistical analysis, and when is\\rit used?\\nHow do you handle a situation where the data has \\noutliers?\\nExplain the concept of 'bias' in machine learning \\nmodels.\\nHyperparameter tuning involves finding the best set of \\nhyperparameters for a\\rmachine learning model to \\noptimise its performance and generalisation.\\nANOVA is used to analyse the differences among group \\nmeans and is applied\\rwhen comparing means of more \\nthan two groups to determine whether they are \\nstatistically\\rsignificantly different.\\nOutliers can be handled by removing them if they are \\ndue to data entry errors or by\\rtransforming them using \\ntechniques such as winsorization or log transformation.\\nBias refers to the error introduced by approximating a \\nreal-world problem, often\\rdue to oversimplification of \\nthe model. High bias can lead to underfitting.\\nQ.71\\nQ.72\\nQ.73\\nQ.74\"),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content=\"C u r a t e d  b y\\nWhat is the purpose of the 'mean squared error' \\nmetric in regression analysis?\\nCan you explain the purpose of the term 'cosine \\nsimilarity' in similarity measurements?\\nHow do you handle a situation where the data has \\na time component?\\nMean squared error is a commonly used metric for \\nevaluating the performance of a\\rregression model by \\nmeasuring the average of the squares of the \\ndifferences between\\rpredicted and actual values.\\nCosine similarity is a metric used to measure the \\nsimilarity between two non-zero\\rvectors, often used in \\ntext mining and collaborative filtering.\\nData with a time component can be analysed using \\ntime series analysis techniques\\rsuch as\\rautoregressive \\nintegrated moving average (ARIMA) models, \\nexponential smoothing, or\\rProphet forecasting models.\\r\\nQ.75\\nQ.76\\nQ.77\"),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content=\"C u r a t e d  b y\\nExplain the concept of 'precision' and 'recall' in the \\ncontext of classification models.\\nWhat is the purpose of the 'Hadoop' framework in \\nbig data processing, and how is it\\rused?\\nHow do you handle a situation where the data has \\na lot of noise?\\nPrecision measures the proportion of true positive \\nresults among the predicted\\rpositive results, while \\nrecall measures the proportion of true positive results \\namong the\\ractual positive results.\\nHadoop is an open-source framework used for \\ndistributed storage and processing\\rof large data sets \\nacross clusters of computers using simple \\nprogramming models.\\nNoisy data can be managed through techniques such \\nas data smoothing, filtering,\\ror by using robust \\nstatistical measures that are less sensitive to outliers.\\nQ.78\\nQ.79\\nQ.80\"),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content=\"C u r a t e d  b y\\nExplain the concept of 'correlation' in statistics and \\nits different types.\\nWhat is the purpose of the 'k-nearest neighbours' \\nalgorithm in machine learning, and\\rhow does it \\nwork?\\nHow do you handle a situation where the data has \\na lot of categorical variables?\\nCorrelation measures the relationship between two \\nvariables and can be positive,\\rnegative, or zero, \\nindicating the strength and direction of the \\nrelationship.\\r\\nThe k-nearest neighbours algorithm is used for \\nclassification and regression tasks,\\rmaking predictions \\nbased on the majority vote or averaging the values of \\nthe k nearest\\rneighbours.\\nCategorical variables can be handled through \\ntechniques such as one-hot\\rencoding, label encoding, \\nor using target encoding to convert them into a format \\nsuitable for\\rmachine learning models.\\nQ.81\\nQ.82\\nQ.83\"),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content=\"t u t o r t  a c a d e m yC u r a t e d  b y\\nExplain the purpose of the 'SVM' (Support Vector \\nMachine) algorithm in machine\\rlearning, and its \\nadvantages.\\nSupport Vector Machines are supervised learning \\nmodels used for classification\\rand regression analysis. \\nThey are effective in high-dimensional spaces and \\nwork well with\\rcomplex datasets.\\nQ.84\\nWhat is the purpose of the 'LSTM' \\n(Long Short-Term Memory) network in deep \\nlearning,\\rand how is it used?\\nCan you explain the purpose of the term 'Principal \\nComponent Analysis' (PCA)\\rin dimensionality \\nreduction, and how is it used?\\nLSTM networks are a type of recurrent neural network \\n(RNN) used for processing\\rand making predictions \\nbased on sequential data, often used in natural \\nlanguage processing\\rand time series analysis.\\nPrincipal Component Analysis is a technique used to \\nreduce the dimensionality of a\\rdataset while \\npreserving as much variance as possible. It transforms \\nthe original variables\\rinto a new set of variables, the \\nprincipal components, which are orthogonal and\\r\\nuncorrelated. This aids in simplifying the dataset and \\nspeeding up the subsequent learning\\ralgorithms while \\nretaining most of the essential information.\\nQ.85\\nQ.86\"),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content=\"C u r a t e d  b y\\nExplain the concept of 'k-means clustering' and its \\napplication in unsupervised learning.\\nWhat is the purpose of the 'R-squared' metric in \\nregression analysis, and what does it\\rindicate \\nabout the model's fit?\\nWhat is the purpose of the term 't-Distributed \\nStochastic Neighbour Embedding' (t-SNE)\\r\\nin dimensionality reduction, and how is it used?\\nK-means clustering is a popular unsupervised learning \\nalgorithm used for\\rpartitioning a dataset into K clusters \\nbased on similarities in the data points.\\nR-squared is a statistical measure that represents the \\nproportion of the variance for\\ra dependent variable \\nexplained by the independent variables in a regression \\nmodel. It\\rindicates the goodness of fit of the model.\\nt-Distributed Stochastic Neighbour Embedding is a \\nnonlinear dimensionality\\rreduction technique used for \\nvisualising high-dimensional data in a low-\\ndimensional space. It\\ris particularly useful for \\nvisualising complex datasets and identifying patterns \\nor clusters\\rwithin the data.\\nQ.87\\nQ.88\\nQ.89\"),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content=\"C u r a t e d  b y\\nExplain the purpose of the 'F1 score' metric in \\nevaluating classification models and its\\r\\nrelationship with precision and recall.\\nCan you explain the concept of 'backpropagation' \\nin neural networks and its role in\\rtraining the \\nmodel?\\nThe F1 score is the harmonic mean of precision and \\nrecall and is used to evaluate\\rthe balance between \\nprecision and recall in a classification model.\\r\\nBackpropagation is an algorithm used to train artificial \\nneural networks by adjusting\\rthe weights of the \\nconnections in the network to minimise the difference \\nbetween predicted\\rand actual outputs.\\nQ.90\\nQ.91\\nWhat is the purpose of the 'chi-square test' in \\nstatistics, and when is it used?\\nThe chi-square test is used to determine the \\nindependence of two categorical\\rvariables and is often \\nused to test the significance of relationships between \\nvariables in a\\rcontingency table.\\r\\nQ.92\"),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content=\"C u r a t e d  b y\\nHow do you handle a situation where the data is \\nnot normally distributed?\\nExplain the concept of 'latent variables' in the \\ncontext of factor analysis and its\\rimportance.\\nWhat is the purpose of the 'Gini index' in decision \\ntrees, and how is it used in the context\\rof building \\nthe tree?\\nNon-normally distributed data can be transformed \\nusing techniques such as the\\rBox-Cox transformation, \\nYeo-Johnson transformation, or log transformation to \\napproximate a\\rnormal distribution.\\nLatent variables are variables that are not directly \\nobserved but are inferred from\\robserved variables. \\nThey are crucial for capturing underlying factors and \\nreducing the\\rdimensionality of the data.\\nThe Gini index is a metric used to measure the impurity \\nof a node in a decision\\rtree. It is used to find the best \\nsplit for creating a more accurate decision tree.\\nQ.93\\nQ.94\\nQ.95\"),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content=\"t u t o r t  a c a d e m yC u r a t e d  b y\\nHow do you handle a situation where the data has \\na lot of continuous variables?\\nExplain the purpose of 'association rules' in data \\nmining, and provide an example of its\\rapplication.\\nWhat is the purpose of the 'logistic function' in \\nlogistic regression, and how is it used for\\r\\nbinary classification?\\nContinuous variables can be handled through \\ntechniques such as scaling and\\rnormalisation to \\nensure that the variables are on a similar scale, \\npreventing certain features\\rfrom dominating the \\nlearning process.\\nAssociation rules are used to discover interesting \\nrelationships between variables\\rin large datasets. An \\nexample is market basket analysis used to identify \\nproducts frequently\\rpurchased together.\\nThe logistic function is used to model the probability of \\na binary outcome. It maps\\rany real-valued number to \\na value between 0 and 1, making it suitable for binary\\r\\nclassification tasks.\\nQ.96\\nQ.97\\nQ.98\"),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content=\"C u r a t e d  b y\\nHow do you handle a situation where the data has \\na lot of missing values?\\nExplain the concept of 'bagging' and 'boosting' in \\nensemble learning, and provide an\\r\\nexample of when each technique is used.\\nData with missing values can be managed through \\ntechniques such as imputation,\\rusing algorithms like \\nK-Nearest Neighbours, decision trees, or employing \\nadvanced\\rtechniques like deep learning-based \\nimputation.\\nBagging involves training multiple models \\nindependently and combining their\\rpredictions, while \\nboosting trains models sequentially, giving more \\nweight to misclassified\\rdata points. Bagging is used for \\nreducing variance, while boosting is used for reducing \\nbias\\rin ensemble models.\\nQ.99\\nQ.100\\nHighest \\nCTC\\nHiring\\nPartners350+Career \\nTransitions1250+ 2.1CR\\nWhy Tutort Academy?\")]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "951a0631",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e642e45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Text into Chunks\n",
    "def text_split(filtered_docs):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "    texts_chunks = text_splitter.split_documents(filtered_docs)\n",
    "    return texts_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "22ae47b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of docs: 72\n"
     ]
    }
   ],
   "source": [
    "text_chunk = text_split(filtered_docs)\n",
    "print(f'number of docs: {len(text_chunk)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5fc30e0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='with These\\n100 Ques tions\\nData Scientist \\nInterview\\nAce your Next'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='C u r a t e d  b y\\nWhat is the role of a data scientist in an \\norganisation?\\nExplain the difference between supervised and \\nunsupervised learning.\\r\\nWhat is cross-validation, and why is it important?\\r\\nA data scientist is responsible for collecting, analysing, \\nand interpreting complex data to help organisations \\nmake informed decisions.\\nSupervised learning uses labelled data for training, \\nwhile unsupervised learning\\rworks with unlabeled \\ndata to find hidden patterns or relationships.'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content=\"Cross-validation is a technique used to assess how \\nwell a model generalises to\\ran independent dataset. It \\nis important for evaluating a model's performance and \\npreventing\\roverfitting.\\r\\nQ.1\\nQ.2\\nQ.3\"),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='C u r a t e d  b y\\nCan you explain the steps involved in the \\ndata preprocessing process?\\r\\nWhat are some common algorithms used in \\nmachine learning?\\r\\nData preprocessing includes data cleaning, handling \\nmissing values, data\\rtransformation, normalisation, \\nand standardisation to prepare the data for analysis \\nand\\rmodelling.\\r\\nCommon machine learning algorithms include linear \\nregression, logistic\\rregression, decision trees, random \\nforests, support vector machines, and neural networks.\\nQ.4\\nQ.5'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='Q.4\\nQ.5\\nHow do you handle missing data in a dataset?\\nMissing data can be handled by either removing the \\nrows with missing values,\\rimputing the missing values \\nusing statistical techniques, or using advanced \\nimputation\\rmethods such as K-Nearest Neighbors.\\nQ.6'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content=\"C u r a t e d  b y\\nWhat is the purpose of the K-Means clustering \\nalgorithm?\\r\\nHow do you assess the performance of a machine \\nlearning model?\\r\\nExplain the term 'bias' in the context of machine \\nlearning models.\\r\\nWhat is the importance of feature scaling in \\nmachine learning?\\nThe K-Means algorithm is used for partitioning a \\ndataset into K clusters, aiming to\\rminimise the sum of \\nsquares within each cluster.\\r\\nModel performance can be assessed using metrics\"),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='such as accuracy, precision,\\rrecall, F1 score, and the \\nROC curve for classification tasks, and metrics such as \\nmean\\rsquared error for regression tasks.\\nBias refers to the error introduced by approximating a \\nreal-world problem, often\\rdue to oversimplification of \\nthe model. High bias can result in underfitting.\\r\\n Feature scaling ensures that the features are at a \\nsimilar scale, preventing\\rcertain features from \\ndominating the learning process and helping the \\nalgorithm converge\\rfaster.\\nQ.7'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='Q.7\\nQ.8\\nQ.9\\nQ.10'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='C u r a t e d  b y\\nCan you explain the concept of regularisation in \\nmachine learning?\\r\\nWhat is the difference between L1 and L2 \\nregularisation?\\nRegularisation is a technique used to prevent \\noverfitting by adding a penalty term\\rto the loss \\nfunction, discouraging complex models.\\nL1 regularisation adds the absolute value of the \\nmagnitude of coefficients as a\\rpenalty term, while L2 \\nregularisation adds the square of the magnitude of \\ncoefficients as a\\rpenalty term.\\r\\nQ.11\\nQ.12'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='Q.11\\nQ.12\\nWhat is the purpose of a confusion matrix in \\nclassification tasks?\\r\\nA confusion matrix is used to visualise the performance \\nof a classification model,\\rshowing the counts of true \\npositive, true negative, false positive, and false \\nnegative\\rpredictions.\\nQ.13'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='C u r a t e d  b y\\nHow do you handle multicollinearity in a dataset?\\nCan you explain the difference between precision \\nand recall?\\nWhat is the purpose of the Naive Bayes algorithm \\nin machine learning?\\nMulticollinearity  can be handled by \\ntechniques such as removing one of \\nthe\\rcorrelated features, using principal \\ncomponent analysis, or using \\nregularisation techniques to\\rreduce \\nthe impact of correlated features.\\nPrecision refers to the ratio of correctly predicted'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='positive observations to the\\rtotal predicted positive \\nobservations, while recall refers to the ratio of correctly \\npredicted\\rpositive observations to the total actual \\npositive observations.\\nThe Naive Bayes algorithm is used for classification \\ntasks, based on the Bayes\\rtheorem with the \\nassumption of independence between features.\\nQ.14\\nQ.15\\nQ.16'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='C u r a t e d  b y\\nHow do you handle outliers in a dataset?\\nExplain the concept of the Central Limit Theorem.\\nWhat is the purpose of a decision tree algorithm in \\nmachine learning?\\nCan you explain the concept of ensemble \\nlearning?\\nOutliers can be handled by either removing them if \\nthey are due to data entry\\rerrors, or by transforming \\nthem using techniques such as winsorization or log \\ntransformation.\\nThe Central Limit Theorem states that the sampling'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='distribution of the sample\\rmeans approaches a normal \\ndistribution as the sample size increases, regardless of \\nthe\\rshape of the population distribution.\\nDecision trees are used for both classification and \\nregression tasks, creating a\\rmodel that predicts the \\nvalue of a target variable by learning simple decision \\nrules inferred\\rfrom the data features.\\nEnsemble learning involves combining multiple \\nindividual models to improve the\\roverall performance'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='and predictive power of the learning algorithm.\\nQ.17\\nQ.18\\nQ.19\\nQ.20'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='C u r a t e d  b y\\nWhat is the difference between bagging and \\nboosting?\\nExplain the purpose of the Random Forest \\nalgorithm in machine learning.\\nHow do you select the optimal number of clusters \\nin a K-Means clustering algorithm?\\nBagging involves training each model in the ensemble \\nwith a subset of the data,\\rwhile boosting focuses on \\ntraining each model sequentially, giving more weight \\nto the\\rmisclassified data points.\\nRandom Forest is an ensemble learning method that'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='constructs multiple decision\\rtrees during training and \\noutputs the mode of the classes or the mean \\nprediction of the\\rindividual trees for classification and \\nregression tasks, respectively.\\nThe optimal number of clusters can be determined \\nusing techniques such as the\\relbow method, silhouette \\nscore, or the gap statistic.\\nQ.21\\nQ.22\\nQ.23'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='C u r a t e d  b y\\nWhat is the purpose of the Support Vector Machine \\n(SVM) algorithm?\\nHow do you handle a large volume of data that \\ncannot fit into memory?\\nCan you explain the purpose of a recommendation \\nsystem?\\nWhat is the purpose of Principal Component \\nAnalysis (PCA) in machine learning?\\nSupport Vector Machines are used for classification \\nand regression analysis, with\\rthe primary goal of \\nfinding the hyperplane that best separates the classes.\\nLarge volumes of data can be handled using'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='techniques such as data streaming,\\rdistributed \\ncomputing frameworks like Hadoop or Spark, and \\ndata compression techniques.\\nRecommendation systems are used to predict and \\nrecommend items or products\\rthat a user may be \\ninterested in, based on their past preferences or \\nbehaviour.\\nPrincipal Component Analysis is used for dimensionality \\nreduction, transforming\\ra large set of variables into a \\nsmaller set of uncorrelated variables while retaining \\nmost of the\\rinformation.\\nQ.24\\nQ.25\\nQ.26\\nQ.27'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='C u r a t e d  b y\\nHow do you handle a situation where the data is \\ntoo imbalanced?\\nWhat is the purpose of a Recurrent Neural Network \\n(RNN) in deep learning?\\nExplain the concept of a Long Short-Term Memory \\n(LSTM) network.\\nImbalanced data can be handled using techniques \\nsuch as oversampling the\\rminority class, \\nundersampling the majority class, or using algorithms \\nspecifically designed to\\rhandle imbalanced datasets.\\nRecurrent Neural Networks are used for sequence data,'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='allowing information to\\rpersist over time, making them \\nsuitable for tasks such as natural language processing \\nand\\rtime series analysis.\\nLSTM networks are a type of RNN that addresses the \\nvanishing gradient\\rproblem, making them more \\neffective for learning and predicting sequences of data.\\nQ.28\\nQ.29\\nQ.30'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='C u r a t e d  b y\\nWhat is the purpose of the Word2Vec algorithm in \\nnatural language processing?\\nHow do you handle a situation where there are \\ntoo many features compared to the\\rnumber of \\nobservations?\\nExplain the concept of a support vector in the \\ncontext of a Support Vector Machine\\ralgorithm.\\nWord2Vec is used for learning word embeddings, \\nrepresenting words as vectors\\rto capture semantic \\nrelationships between words in a text corpus.\\nThe situation of having too many features compared'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='to the number of\\robservations can be handled by \\nusing feature selection techniques, such as Lasso\\r\\nregression, or by using dimensionality reduction \\ntechniques like PCA or t-SNE.\\r\\nSupport vectors are data points that lie closest to the \\ndecision boundary between\\rthe classes, influencing the \\nposition and orientation of the hyperplane in a Support \\nVector\\rMachine.\\nQ.31\\nQ.32\\nQ.33'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='C u r a t e d  b y\\nWhat is the purpose of the Root Mean Square Error \\n(RMSE) metric in regression tasks?\\nCan you explain the purpose of the Apriori \\nalgorithm in association rule mining?\\nHow do you handle a situation where the data is \\nhighly skewed?\\nThe Root Mean Square Error is a commonly used metric \\nfor evaluating the\\raccuracy of a regression model by \\nmeasuring the differences between the predicted \\nvalues\\rand the actual values.\\nThe Apriori algorithm is used for discovering frequent'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='itemsets within a\\rtransactional database and is \\ncommonly employed in market basket analysis to \\nidentify\\rpatterns or relationships between different \\nitems.\\nHighly skewed data can be handled by using \\ntransformations such as log\\rtransformations, square \\nroot transformations, or by using specialised models \\nthat can handle\\rskewed data more effectively.\\nQ.34\\nQ.35\\nQ.36'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='C u r a t e d  b y\\nWhat is the purpose of the Mean Average Precision \\n(MAP) metric in evaluating\\rinformation retrieval \\nsystems?\\nExplain the purpose of the Euclidean distance \\nmetric in clustering tasks.\\nHow do you handle a situation where the data is \\nnot linearly separable?\\nMean Average Precision is used to evaluate the \\nperformance of information\\rretrieval systems, \\nmeasuring the average precision at each relevant \\ndocument retrieved\\racross multiple queries.'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='The Euclidean distance metric is used to measure the \\ndistance between two\\rpoints in a multidimensional \\nspace and is commonly used in clustering algorithms \\nsuch as\\rK-Means.\\nIn cases where the data is not linearly separable, kernel \\nfunctions can be used in\\ralgorithms like Support Vector \\nMachines to map the data to a higher-dimensional \\nspace\\rwhere it becomes linearly separable.\\nQ.37\\nQ.38\\nQ.39'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='C u r a t e d  b y\\nWhat is the purpose of the Chi-square test in \\nfeature selection?\\nCan you explain the purpose of the Gradient \\nDescent algorithm in machine learning?\\nHow do you handle a situation where the data is \\ntime-series data?\\nThe Chi-square test is used to determine the \\nindependence of two categorical\\rvariables, making it \\nsuitable for feature selection in classification tasks.\\nGradient Descent is an optimization algorithm used to \\nminimise the cost function\\rand find the optimal'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='parameters of a model by iteratively updating the \\nparameters in the\\rdirection of the steepest descent.\\nTime-series data can be handled using techniques \\nsuch as autoregressive\\rintegrated moving average \\n(ARIMA) models, exponential smoothing methods, or \\nmore\\radvanced deep learning models like Long Short-\\nTerm Memory (LSTM) networks.\\nQ.40\\nQ.41\\nQ.42'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='C u r a t e d  b y\\nWhat is the purpose of the K-Nearest Neighbors \\n(KNN) algorithm in machine learning?\\nExplain the purpose of the Log Loss metric in \\nevaluating classification models.\\nHow do you handle a situation where the data is \\nhigh-dimensional?\\nThe K-Nearest Neighbors algorithm is used for \\nclassification and regression\\rtasks, making predictions \\nbased on the majority vote of its k nearest neighbours.\\nLog Loss is used to evaluate the performance of a'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='classification model that\\routputs probabilities, \\nmeasuring the performance based on the likelihood of \\nthe predicted\\rprobabilities matching the actual labels.\\nHigh-dimensional data can be handled by using \\ndimensionality reduction\\rtechniques such as Principal \\nComponent Analysis (PCA), t-Distributed Stochastic \\nNeighbour\\rEmbedding (t-SNE), or by employing feature \\nselection methods.\\nQ.43\\nQ.44\\nQ.45'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='C u r a t e d  b y\\nWhat is the purpose of the R-squared (R2) metric \\nin evaluating regression models?\\nCan you explain the purpose of the Gini index in \\nthe context of a decision tree\\ralgorithm?\\nHow do you handle a situation where there is noise \\nin the data?\\r\\nR-squared is a statistical measure that represents the \\nproportion of the variance\\rfor a dependent variable \\nthat is explained by an independent variable in a \\nregression model.\\nThe Gini index is used to measure the impurity or the'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='homogeneity of a node in a\\rdecision tree, helping to \\ndetermine the best split for creating a more accurate \\ndecision tree.\\r\\nNoise in the data can be handled by smoothing \\ntechniques such as moving\\raverages, using robust \\nstatistics, or employing filtering methods to remove \\noutliers and\\rirrelevant data points.\\nQ.46\\nQ.47\\nQ.48'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='C u r a t e d  b y\\nWhat is the purpose of the F1 score metric in \\nevaluating classification models?\\nWhat is the difference between classification and \\nregression in machine learning?\\nCan you explain the bias-variance trade-off in the \\ncontext of model complexity?\\nCan you explain the purpose of the LDA \\n(Linear Discriminant Analysis) algorithm in\\r\\nmachine learning?\\nThe F1 score is the harmonic mean of precision and \\nrecall and is used to\\revaluate the balance between'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content=\"precision and recall in a classification model.\\nClassification is used to predict discrete categories, \\nwhile regression is used to\\rpredict continuous \\nquantities.\\r\\nThe bias-variance trade-off highlights the trade-off \\nbetween a model's ability to\\rminimise errors due to \\nbias and variance. Increasing model complexity \\nreduces bias but\\rincreases variance and vice versa.\\nLinear Discriminant Analysis is used for dimensionality \\nreduction and\\rclassification tasks, aiming to find the\"),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='linear combinations of features that best separate\\r\\nmultiple classes in the data.\\nQ.49\\nQ.51\\nQ.52\\nQ.50'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content=\"C u r a t e d  b y\\nHow do you handle imbalanced data sets when \\nbuilding a classification model?\\nExplain the purpose of the term 'regularisation' in \\nmachine learning models.\\nHow do you assess the performance of a \\nclassification model apart from accuracy?\\nWhat is the purpose of the term 'gradient descent' \\nin the context of optimising a model?\\nImbalanced datasets can be handled using techniques \\nlike oversampling,\\rundersampling, or using algorithms \\ndesigned for imbalanced data such as SMOTE\"),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content=\"(Synthetic Minority Over-sampling Technique).\\nRegularisation is a technique used to prevent \\noverfitting by adding a penalty term\\rto the loss \\nfunction, discouraging overly complex models.\\nThe performance of a classification model can be \\nevaluated using metrics such as\\rprecision, recall, F1 \\nscore, and the area under the ROC curve.\\nGradient descent is an iterative optimization algorithm \\nused to minimise the cost\\rfunction of a model by \\nadjusting the model's parameters in the direction of\"),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='steepest descent.\\r\\nQ.53\\nQ.54\\nQ.56\\nQ.55'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content=\"C u r a t e d  b y\\nCan you explain the concept of 'feature selection' \\nand its importance in model building?\\nWhat is the purpose of the term 'cross-validation' \\nin model training and evaluation?\\nHow do you handle missing data in a dataset while \\nbuilding a predictive model?\\nFeature selection involves selecting the most relevant \\nfeatures from a dataset. It is\\rcrucial for improving \\nmodel performance, reducing overfitting, and \\nenhancing interpretability.\"),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content=\"Cross-validation is used to assess how well a model \\ngeneralises to an\\rindependent dataset, minimising the \\nrisk of overfitting and providing a more accurate\\r\\nestimate of the model's performance.\\nMissing data can be handled by \\ntechniques such as mean/median \\nimputation,\\rmode imputation, or \\nusing advanced methods like \\nmultiple imputation or K-Nearest\\r\\nNeighbors imputation.\\nQ.57\\nQ.58\\nQ.59\"),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content=\"C u r a t e d  b y\\nExplain the purpose of the term 'ensemble learning' \\nand its benefits in model building.\\nWhat is the difference between unsupervised and \\nsupervised machine learning\\ralgorithms?\\nCan you explain the concept of 'clustering' and \\nprovide an example of when it is used?\\nWhat is the purpose of 'dimensionality reduction' in \\ndata analysis, and how is it\\rachieved?\\nEnsemble learning involves combining multiple models \\nto improve predictive\\rperformance and reduce\"),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='overfitting, often resulting in better generalisation and \\nmore robust\\rpredictions.\\r\\nSupervised learning uses labelled data for training, \\nwhile unsupervised learning\\rworks with unlabeled data \\nto find patterns and relationships.\\nClustering is an unsupervised learning technique used \\nto group similar data points\\rtogether. An example is \\ncustomer segmentation in marketing.\\nDimensionality reduction is used to reduce the number \\nof features in a dataset. It is\\rachieved through'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='techniques like principal component analysis (PCA) \\nand t-distributed\\rstochastic neighbour embedding (t-\\nSNE).\\r\\nQ.60\\nQ.61\\nQ.62\\nQ.63'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content=\"C u r a t e d  b y\\nHow do you handle the problem of overfitting in \\nmachine learning models?\\nHow do you handle the problem of multicollinearity \\nin a dataset?\\nExplain the purpose of the term 'Naive Bayes' in \\nmachine learning and its application.\\nWhat is the purpose of the term 'decision trees' in \\nmachine learning, and how does it\\rwork?\\nOverfitting can be mitigated by using techniques like \\ncross-validation,\\rregularisation, early stopping, and \\nreducing model complexity.\"),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content=\"Multicollinearity can be addressed by techniques such \\nas removing one of the\\rcorrelated features, using \\nprincipal component analysis (PCA), or using \\nregularisation\\rmethods.\\nNaive Bayes is a probabilistic classification algorithm \\nbased on Bayes' theorem\\rwith an assumption of \\nindependence between features. It is commonly used \\nfor text\\rclassification and spam filtering.\\nDecision trees are predictive models that map features \\nto conclusions about the\\rtarget value. They work by\"),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='splitting the dataset into smaller subsets based on the \\nmost\\rsignificant differentiators in the data.\\nQ.64\\nQ.67\\nQ.65\\nQ.66'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content=\"C u r a t e d  b y\\nCan you explain the purpose of the term 'random \\nforest' in machine learning and its\\radvantages?\\nWhat is the purpose of 'data preprocessing' in \\nmachine learning, and what are some\\rcommon \\ntechniques used?\\nHow do you handle the problem of underfitting in a \\nmachine learning model?\\nRandom forests are an ensemble learning method that \\nconstructs multiple decision\\rtrees during training. They \\nare effective for reducing overfitting and handling large\"),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='datasets\\rwith high dimensionality.\\nData preprocessing involves preparing and cleaning \\ndata before it is fed into a\\rmachine learning model. \\nCommon techniques include data normalisation, \\nstandardisation,\\rand handling missing values.\\nUnderfitting can be addressed by using more complex \\nmodels, adding more\\rfeatures, or reducing \\nregularisation, allowing the model to capture more \\ncomplex patterns in\\rthe data.\\nQ.68\\nQ.69\\nQ.70'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content=\"C u r a t e d  b y\\nExplain the concept of 'hyperparameter tuning' in \\nmachine learning algorithms.\\nWhat is the purpose of 'ANOVA' (Analysis of \\nVariance) in statistical analysis, and when is\\rit used?\\nHow do you handle a situation where the data has \\noutliers?\\nExplain the concept of 'bias' in machine learning \\nmodels.\\nHyperparameter tuning involves finding the best set of \\nhyperparameters for a\\rmachine learning model to \\noptimise its performance and generalisation.\"),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='ANOVA is used to analyse the differences among group \\nmeans and is applied\\rwhen comparing means of more \\nthan two groups to determine whether they are \\nstatistically\\rsignificantly different.\\nOutliers can be handled by removing them if they are \\ndue to data entry errors or by\\rtransforming them using \\ntechniques such as winsorization or log transformation.\\nBias refers to the error introduced by approximating a \\nreal-world problem, often\\rdue to oversimplification of'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='the model. High bias can lead to underfitting.\\nQ.71\\nQ.72\\nQ.73\\nQ.74'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content=\"C u r a t e d  b y\\nWhat is the purpose of the 'mean squared error' \\nmetric in regression analysis?\\nCan you explain the purpose of the term 'cosine \\nsimilarity' in similarity measurements?\\nHow do you handle a situation where the data has \\na time component?\\nMean squared error is a commonly used metric for \\nevaluating the performance of a\\rregression model by \\nmeasuring the average of the squares of the \\ndifferences between\\rpredicted and actual values.\"),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='Cosine similarity is a metric used to measure the \\nsimilarity between two non-zero\\rvectors, often used in \\ntext mining and collaborative filtering.\\nData with a time component can be analysed using \\ntime series analysis techniques\\rsuch as\\rautoregressive \\nintegrated moving average (ARIMA) models, \\nexponential smoothing, or\\rProphet forecasting models.\\r\\nQ.75\\nQ.76\\nQ.77'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content=\"C u r a t e d  b y\\nExplain the concept of 'precision' and 'recall' in the \\ncontext of classification models.\\nWhat is the purpose of the 'Hadoop' framework in \\nbig data processing, and how is it\\rused?\\nHow do you handle a situation where the data has \\na lot of noise?\\nPrecision measures the proportion of true positive \\nresults among the predicted\\rpositive results, while \\nrecall measures the proportion of true positive results \\namong the\\ractual positive results.\"),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='Hadoop is an open-source framework used for \\ndistributed storage and processing\\rof large data sets \\nacross clusters of computers using simple \\nprogramming models.\\nNoisy data can be managed through techniques such \\nas data smoothing, filtering,\\ror by using robust \\nstatistical measures that are less sensitive to outliers.\\nQ.78\\nQ.79\\nQ.80'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content=\"C u r a t e d  b y\\nExplain the concept of 'correlation' in statistics and \\nits different types.\\nWhat is the purpose of the 'k-nearest neighbours' \\nalgorithm in machine learning, and\\rhow does it \\nwork?\\nHow do you handle a situation where the data has \\na lot of categorical variables?\\nCorrelation measures the relationship between two \\nvariables and can be positive,\\rnegative, or zero, \\nindicating the strength and direction of the \\nrelationship.\\r\\nThe k-nearest neighbours algorithm is used for\"),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='classification and regression tasks,\\rmaking predictions \\nbased on the majority vote or averaging the values of \\nthe k nearest\\rneighbours.\\nCategorical variables can be handled through \\ntechniques such as one-hot\\rencoding, label encoding, \\nor using target encoding to convert them into a format \\nsuitable for\\rmachine learning models.\\nQ.81\\nQ.82\\nQ.83'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content=\"t u t o r t  a c a d e m yC u r a t e d  b y\\nExplain the purpose of the 'SVM' (Support Vector \\nMachine) algorithm in machine\\rlearning, and its \\nadvantages.\\nSupport Vector Machines are supervised learning \\nmodels used for classification\\rand regression analysis. \\nThey are effective in high-dimensional spaces and \\nwork well with\\rcomplex datasets.\\nQ.84\\nWhat is the purpose of the 'LSTM' \\n(Long Short-Term Memory) network in deep \\nlearning,\\rand how is it used?\"),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content=\"Can you explain the purpose of the term 'Principal \\nComponent Analysis' (PCA)\\rin dimensionality \\nreduction, and how is it used?\\nLSTM networks are a type of recurrent neural network \\n(RNN) used for processing\\rand making predictions \\nbased on sequential data, often used in natural \\nlanguage processing\\rand time series analysis.\\nPrincipal Component Analysis is a technique used to \\nreduce the dimensionality of a\\rdataset while \\npreserving as much variance as possible. It transforms\"),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='the original variables\\rinto a new set of variables, the \\nprincipal components, which are orthogonal and\\r\\nuncorrelated. This aids in simplifying the dataset and \\nspeeding up the subsequent learning\\ralgorithms while \\nretaining most of the essential information.\\nQ.85\\nQ.86'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content=\"C u r a t e d  b y\\nExplain the concept of 'k-means clustering' and its \\napplication in unsupervised learning.\\nWhat is the purpose of the 'R-squared' metric in \\nregression analysis, and what does it\\rindicate \\nabout the model's fit?\\nWhat is the purpose of the term 't-Distributed \\nStochastic Neighbour Embedding' (t-SNE)\\r\\nin dimensionality reduction, and how is it used?\\nK-means clustering is a popular unsupervised learning \\nalgorithm used for\\rpartitioning a dataset into K clusters\"),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='based on similarities in the data points.\\nR-squared is a statistical measure that represents the \\nproportion of the variance for\\ra dependent variable \\nexplained by the independent variables in a regression \\nmodel. It\\rindicates the goodness of fit of the model.\\nt-Distributed Stochastic Neighbour Embedding is a \\nnonlinear dimensionality\\rreduction technique used for \\nvisualising high-dimensional data in a low-\\ndimensional space. It\\ris particularly useful for'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='visualising complex datasets and identifying patterns \\nor clusters\\rwithin the data.\\nQ.87\\nQ.88\\nQ.89'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content=\"C u r a t e d  b y\\nExplain the purpose of the 'F1 score' metric in \\nevaluating classification models and its\\r\\nrelationship with precision and recall.\\nCan you explain the concept of 'backpropagation' \\nin neural networks and its role in\\rtraining the \\nmodel?\\nThe F1 score is the harmonic mean of precision and \\nrecall and is used to evaluate\\rthe balance between \\nprecision and recall in a classification model.\\r\\nBackpropagation is an algorithm used to train artificial\"),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content=\"neural networks by adjusting\\rthe weights of the \\nconnections in the network to minimise the difference \\nbetween predicted\\rand actual outputs.\\nQ.90\\nQ.91\\nWhat is the purpose of the 'chi-square test' in \\nstatistics, and when is it used?\\nThe chi-square test is used to determine the \\nindependence of two categorical\\rvariables and is often \\nused to test the significance of relationships between \\nvariables in a\\rcontingency table.\\r\\nQ.92\"),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content=\"C u r a t e d  b y\\nHow do you handle a situation where the data is \\nnot normally distributed?\\nExplain the concept of 'latent variables' in the \\ncontext of factor analysis and its\\rimportance.\\nWhat is the purpose of the 'Gini index' in decision \\ntrees, and how is it used in the context\\rof building \\nthe tree?\\nNon-normally distributed data can be transformed \\nusing techniques such as the\\rBox-Cox transformation, \\nYeo-Johnson transformation, or log transformation to \\napproximate a\\rnormal distribution.\"),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='Latent variables are variables that are not directly \\nobserved but are inferred from\\robserved variables. \\nThey are crucial for capturing underlying factors and \\nreducing the\\rdimensionality of the data.\\nThe Gini index is a metric used to measure the impurity \\nof a node in a decision\\rtree. It is used to find the best \\nsplit for creating a more accurate decision tree.\\nQ.93\\nQ.94\\nQ.95'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content=\"t u t o r t  a c a d e m yC u r a t e d  b y\\nHow do you handle a situation where the data has \\na lot of continuous variables?\\nExplain the purpose of 'association rules' in data \\nmining, and provide an example of its\\rapplication.\\nWhat is the purpose of the 'logistic function' in \\nlogistic regression, and how is it used for\\r\\nbinary classification?\\nContinuous variables can be handled through \\ntechniques such as scaling and\\rnormalisation to \\nensure that the variables are on a similar scale,\"),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='preventing certain features\\rfrom dominating the \\nlearning process.\\nAssociation rules are used to discover interesting \\nrelationships between variables\\rin large datasets. An \\nexample is market basket analysis used to identify \\nproducts frequently\\rpurchased together.\\nThe logistic function is used to model the probability of \\na binary outcome. It maps\\rany real-valued number to \\na value between 0 and 1, making it suitable for binary\\r\\nclassification tasks.\\nQ.96\\nQ.97\\nQ.98'),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content=\"C u r a t e d  b y\\nHow do you handle a situation where the data has \\na lot of missing values?\\nExplain the concept of 'bagging' and 'boosting' in \\nensemble learning, and provide an\\r\\nexample of when each technique is used.\\nData with missing values can be managed through \\ntechniques such as imputation,\\rusing algorithms like \\nK-Nearest Neighbours, decision trees, or employing \\nadvanced\\rtechniques like deep learning-based \\nimputation.\\nBagging involves training multiple models\"),\n",
       " Document(metadata={'source': 'data\\\\datascience.pdf'}, page_content='independently and combining their\\rpredictions, while \\nboosting trains models sequentially, giving more \\nweight to misclassified\\rdata points. Bagging is used for \\nreducing variance, while boosting is used for reducing \\nbias\\rin ensemble models.\\nQ.99\\nQ.100\\nHighest \\nCTC\\nHiring\\nPartners350+Career \\nTransitions1250+ 2.1CR\\nWhy Tutort Academy?')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "16c4e298",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_17808\\1978270023.py:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
      "c:\\Users\\HP\\Desktop\\Aditya Sharma\\Aditya\\Chatbot\\Medical-Chatbot\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\HP\\Desktop\\Aditya Sharma\\Aditya\\Chatbot\\Medical-Chatbot\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\HP\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings, HuggingFaceEmbeddings\n",
    "\n",
    "def download_embeddings():\n",
    "    model_name = 'sentence-transformers/all-MiniLM-L6-v2'\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
    "    return embeddings\n",
    "\n",
    "embedding = download_embeddings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c2f9f7f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbeddings(client=SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False, 'architecture': 'BertModel'})\n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       "), model_name='sentence-transformers/all-MiniLM-L6-v2', cache_folder=None, model_kwargs={}, encode_kwargs={}, multi_process=False, show_progress=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fe181481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.031149033457040787,\n",
       " 0.03860113397240639,\n",
       " 0.015983236953616142,\n",
       " 0.026156900450587273,\n",
       " -0.04433733969926834,\n",
       " -0.1398538500070572,\n",
       " 0.09821799397468567,\n",
       " 0.024140600115060806,\n",
       " -0.01106710359454155,\n",
       " 0.03463781252503395,\n",
       " 0.051626723259687424,\n",
       " 0.0179996769875288,\n",
       " 0.039424456655979156,\n",
       " -0.022039974108338356,\n",
       " -0.0298737995326519,\n",
       " -0.028813710436224937,\n",
       " 0.016378799453377724,\n",
       " -0.022750819101929665,\n",
       " -0.15493959188461304,\n",
       " -0.039053093641996384,\n",
       " -0.02454957738518715,\n",
       " 0.06758945435285568,\n",
       " -0.01739676482975483,\n",
       " -0.000950572662986815,\n",
       " -0.11589008569717407,\n",
       " -0.029670866206288338,\n",
       " 0.057712189853191376,\n",
       " 0.04777739569544792,\n",
       " 0.006865103263407946,\n",
       " -0.0564885213971138,\n",
       " 0.01824294589459896,\n",
       " 0.010030942969024181,\n",
       " 0.12577754259109497,\n",
       " -0.001553717302158475,\n",
       " 0.006623221095651388,\n",
       " 0.06654299050569534,\n",
       " -0.0848105177283287,\n",
       " -0.10550934821367264,\n",
       " 0.026824962347745895,\n",
       " 0.027513356879353523,\n",
       " 0.02594820410013199,\n",
       " -0.10487924516201019,\n",
       " 0.009051548317074776,\n",
       " -0.027972286567091942,\n",
       " 0.00459702406078577,\n",
       " 0.000538822147063911,\n",
       " -0.017102934420108795,\n",
       " 0.05007153004407883,\n",
       " 0.059068892151117325,\n",
       " 0.008529222570359707,\n",
       " -0.10642000287771225,\n",
       " -0.05905677750706673,\n",
       " -0.02113940566778183,\n",
       " 0.0027990899980068207,\n",
       " 0.08873351663351059,\n",
       " 0.003902164753526449,\n",
       " 0.030962811782956123,\n",
       " -0.013101765885949135,\n",
       " 0.019436802715063095,\n",
       " -0.02129632607102394,\n",
       " -0.04096699878573418,\n",
       " 0.03352295234799385,\n",
       " -0.012511194683611393,\n",
       " 0.02329188399016857,\n",
       " 0.10658606141805649,\n",
       " -0.06407435983419418,\n",
       " -0.016468685120344162,\n",
       " 0.03320612385869026,\n",
       " -0.05760958418250084,\n",
       " -0.044457074254751205,\n",
       " -0.06036412715911865,\n",
       " 0.002926130313426256,\n",
       " 0.0016307932091876864,\n",
       " 0.06423990428447723,\n",
       " -0.04779897257685661,\n",
       " 0.01134253665804863,\n",
       " 0.02436540275812149,\n",
       " 0.016670480370521545,\n",
       " 0.0017350924899801612,\n",
       " 0.03983055427670479,\n",
       " 0.037464577704668045,\n",
       " -0.0852755606174469,\n",
       " 0.009967368096113205,\n",
       " 0.018923137336969376,\n",
       " 0.032831352204084396,\n",
       " -0.001804306055419147,\n",
       " 0.020776687189936638,\n",
       " -0.005944888107478619,\n",
       " -0.028218790888786316,\n",
       " -0.028274979442358017,\n",
       " -0.10379967093467712,\n",
       " -0.038159728050231934,\n",
       " -0.019472472369670868,\n",
       " -0.010019796900451183,\n",
       " -0.08889152854681015,\n",
       " -0.02899354137480259,\n",
       " 0.09698358178138733,\n",
       " -0.04706071689724922,\n",
       " -0.11608411371707916,\n",
       " 0.17469926178455353,\n",
       " 0.053178224712610245,\n",
       " 0.040305741131305695,\n",
       " 0.03179521486163139,\n",
       " 0.02619784325361252,\n",
       " 0.018780140206217766,\n",
       " 0.04015549644827843,\n",
       " -0.01893538422882557,\n",
       " 0.06636600196361542,\n",
       " -0.03265931084752083,\n",
       " -0.02224091999232769,\n",
       " -0.02921767346560955,\n",
       " -0.03906431421637535,\n",
       " 0.03644946217536926,\n",
       " 0.017388898879289627,\n",
       " 0.06515319645404816,\n",
       " -0.05461002513766289,\n",
       " -0.03027971275150776,\n",
       " 0.018435604870319366,\n",
       " -0.04115927591919899,\n",
       " -0.01309820543974638,\n",
       " 0.035567790269851685,\n",
       " -0.037114132195711136,\n",
       " -0.0075948191806674,\n",
       " 0.01435840968042612,\n",
       " 0.0036022961139678955,\n",
       " 0.011596047319471836,\n",
       " -0.013152741827070713,\n",
       " -4.520547564421407e-33,\n",
       " 0.06306587159633636,\n",
       " -0.014963716268539429,\n",
       " 0.0012604560470208526,\n",
       " 0.15129899978637695,\n",
       " 0.04690766707062721,\n",
       " 0.011018825694918633,\n",
       " -0.09917576611042023,\n",
       " -0.037376124411821365,\n",
       " 0.02859675884246826,\n",
       " -0.022802188992500305,\n",
       " 0.03027164191007614,\n",
       " 0.04990725591778755,\n",
       " -0.03242676332592964,\n",
       " 0.0340292826294899,\n",
       " -0.03154361993074417,\n",
       " 0.05182090029120445,\n",
       " -0.04075351357460022,\n",
       " 0.0012694726465269923,\n",
       " 0.029669107869267464,\n",
       " 0.09630388021469116,\n",
       " -0.07196507602930069,\n",
       " -0.03420066833496094,\n",
       " 0.02201024256646633,\n",
       " 0.07905614376068115,\n",
       " -0.0016842563636600971,\n",
       " 0.01377197913825512,\n",
       " -0.013179805129766464,\n",
       " -0.10009539872407913,\n",
       " 0.06029218062758446,\n",
       " 0.007005530875176191,\n",
       " -0.0033566628117114305,\n",
       " -0.03676590323448181,\n",
       " 0.027400808408856392,\n",
       " -0.006141259800642729,\n",
       " 0.003195633878931403,\n",
       " -0.008357979357242584,\n",
       " -0.015778271481394768,\n",
       " -0.07747913897037506,\n",
       " -0.08244793862104416,\n",
       " -0.0026197461411356926,\n",
       " -0.07351282984018326,\n",
       " 0.038648225367069244,\n",
       " 0.03500154986977577,\n",
       " -0.017676183953881264,\n",
       " 0.026803920045495033,\n",
       " -0.010187317617237568,\n",
       " 0.018716633319854736,\n",
       " 0.042932841926813126,\n",
       " -0.011516288854181767,\n",
       " 0.04871220514178276,\n",
       " -0.015551535412669182,\n",
       " -0.007229784037917852,\n",
       " -0.06619919836521149,\n",
       " 0.014788711443543434,\n",
       " -0.009520214982330799,\n",
       " 0.0306868739426136,\n",
       " 0.016060948371887207,\n",
       " -0.018011990934610367,\n",
       " 0.02038523554801941,\n",
       " 0.07308848202228546,\n",
       " 0.03730427846312523,\n",
       " 0.08736944198608398,\n",
       " -0.03504941612482071,\n",
       " 0.014466363936662674,\n",
       " -0.02479921281337738,\n",
       " -0.031039830297231674,\n",
       " 0.1049211397767067,\n",
       " 0.017378641292452812,\n",
       " 0.04416649788618088,\n",
       " 0.002918222453445196,\n",
       " -0.05461113527417183,\n",
       " 0.014607332646846771,\n",
       " 0.00724779162555933,\n",
       " 0.05620772764086723,\n",
       " 0.06398899853229523,\n",
       " 0.03342264145612717,\n",
       " 0.0923786610364914,\n",
       " -0.011882898397743702,\n",
       " 0.016497548669576645,\n",
       " -0.06163178011775017,\n",
       " 0.03203652799129486,\n",
       " 0.014823026023805141,\n",
       " 0.03268544003367424,\n",
       " -0.013733896426856518,\n",
       " 0.10947563499212265,\n",
       " -0.008552128449082375,\n",
       " -0.017633216455578804,\n",
       " -0.09018648415803909,\n",
       " -0.010566365905106068,\n",
       " -0.041494373232126236,\n",
       " -0.11962712556123734,\n",
       " 0.07544249296188354,\n",
       " 0.04855770990252495,\n",
       " -0.05871335789561272,\n",
       " -0.10772727429866791,\n",
       " 2.445926400862783e-33,\n",
       " 0.10674609243869781,\n",
       " 0.030375493690371513,\n",
       " -0.015079818665981293,\n",
       " -0.029865998774766922,\n",
       " -0.032363489270210266,\n",
       " -0.0426577627658844,\n",
       " -0.07567691802978516,\n",
       " 0.16531269252300262,\n",
       " -0.07598783820867538,\n",
       " 0.07804501056671143,\n",
       " 0.03185427561402321,\n",
       " 0.005097195506095886,\n",
       " 0.10590927302837372,\n",
       " 0.04256895184516907,\n",
       " 0.08919937163591385,\n",
       " -0.027354519814252853,\n",
       " 0.12109449505805969,\n",
       " 0.015413865447044373,\n",
       " -0.02767486311495304,\n",
       " -0.010729766450822353,\n",
       " -0.030462797731161118,\n",
       " 0.010777870193123817,\n",
       " -0.017071377485990524,\n",
       " -0.019677774980664253,\n",
       " -0.022255420684814453,\n",
       " 0.006335597950965166,\n",
       " 0.039540644735097885,\n",
       " 0.02555500902235508,\n",
       " -0.07336242496967316,\n",
       " 0.0493486188352108,\n",
       " 0.03378206863999367,\n",
       " 0.01829466223716736,\n",
       " -0.07468773424625397,\n",
       " -0.0030523051973432302,\n",
       " -0.014859242364764214,\n",
       " 0.037096910178661346,\n",
       " -0.12920381128787994,\n",
       " -0.026794616132974625,\n",
       " -0.04405437409877777,\n",
       " -0.03506365790963173,\n",
       " -0.08970002084970474,\n",
       " 0.013082792982459068,\n",
       " -0.04665493965148926,\n",
       " 0.024041494354605675,\n",
       " -0.086541548371315,\n",
       " -0.03294952213764191,\n",
       " -0.0806618332862854,\n",
       " 0.017720818519592285,\n",
       " -0.032227758318185806,\n",
       " -0.033045269548892975,\n",
       " -0.07896959036588669,\n",
       " 0.00019395010895095766,\n",
       " 0.031076667830348015,\n",
       " -0.0334378145635128,\n",
       " -0.04104088246822357,\n",
       " 0.030288442969322205,\n",
       " -0.012738906778395176,\n",
       " -0.002686847001314163,\n",
       " 0.027493491768836975,\n",
       " -0.011944850906729698,\n",
       " 0.025768447667360306,\n",
       " 0.04976726323366165,\n",
       " 0.023665165528655052,\n",
       " 0.07576999813318253,\n",
       " -0.013897296041250229,\n",
       " -0.007999862544238567,\n",
       " -0.0007187016890384257,\n",
       " 0.056671369820833206,\n",
       " 0.03231770545244217,\n",
       " -0.04047714173793793,\n",
       " -0.033692438155412674,\n",
       " 0.03019827976822853,\n",
       " -0.0399707667529583,\n",
       " -0.015343652106821537,\n",
       " -0.03279782086610794,\n",
       " 0.06592843681573868,\n",
       " 0.009415049105882645,\n",
       " 0.006730332504957914,\n",
       " 0.026580747216939926,\n",
       " 0.003501835511997342,\n",
       " 0.029091371223330498,\n",
       " 0.02832619845867157,\n",
       " 0.023216815665364265,\n",
       " 0.01619870215654373,\n",
       " 0.016077950596809387,\n",
       " -0.011267034336924553,\n",
       " 0.06824604421854019,\n",
       " 0.05489431694149971,\n",
       " -0.0009533416014164686,\n",
       " -0.06411252915859222,\n",
       " -0.00679566478356719,\n",
       " 0.01648857444524765,\n",
       " -0.02460755780339241,\n",
       " 0.002476249821484089,\n",
       " -0.08264600485563278,\n",
       " -1.7372878247101653e-08,\n",
       " -0.05778740718960762,\n",
       " 0.02405492588877678,\n",
       " -0.007284336257725954,\n",
       " 0.0624593086540699,\n",
       " -0.0077934907749295235,\n",
       " 0.05449119582772255,\n",
       " -0.03184080496430397,\n",
       " -0.06849295645952225,\n",
       " -0.06398749351501465,\n",
       " 0.0032514380291104317,\n",
       " 0.05629013106226921,\n",
       " 0.10153427720069885,\n",
       " -0.03965849056839943,\n",
       " -0.002691605594009161,\n",
       " 0.02268053963780403,\n",
       " 0.019404148682951927,\n",
       " -0.05805901065468788,\n",
       " 0.0002699920441955328,\n",
       " -0.02187914215028286,\n",
       " -0.0020218794234097004,\n",
       " -0.018603667616844177,\n",
       " -0.004534050356596708,\n",
       " 0.0879644900560379,\n",
       " -0.08640177547931671,\n",
       " 0.046609289944171906,\n",
       " -0.03780665248632431,\n",
       " -0.020147021859884262,\n",
       " 0.04542941600084305,\n",
       " -0.0324135348200798,\n",
       " -0.03712306171655655,\n",
       " 0.03909315913915634,\n",
       " 0.1097843274474144,\n",
       " -0.012247342616319656,\n",
       " 0.020389489829540253,\n",
       " -0.10434620082378387,\n",
       " -0.012855305336415768,\n",
       " 0.023403938859701157,\n",
       " 0.08910594880580902,\n",
       " 0.06061061471700668,\n",
       " -0.07125731557607651,\n",
       " -0.0795564204454422,\n",
       " 0.06930850446224213,\n",
       " -0.025325553491711617,\n",
       " -0.06966333836317062,\n",
       " -0.057875391095876694,\n",
       " 0.036532212048769,\n",
       " 0.09665849804878235,\n",
       " -0.04495138302445412,\n",
       " 0.011461406014859676,\n",
       " -0.04420773312449455,\n",
       " -0.10174329578876495,\n",
       " 0.02884831093251705,\n",
       " 0.0930393785238266,\n",
       " 0.02848387323319912,\n",
       " 0.10304919630289078,\n",
       " 0.07272545993328094,\n",
       " -0.008233844302594662,\n",
       " 0.04550847411155701,\n",
       " -0.015600824728608131,\n",
       " 0.04940114542841911,\n",
       " -0.0004812133847735822,\n",
       " -0.012450926005840302,\n",
       " 0.020062468945980072,\n",
       " -0.015419116243720055]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = embedding.embed_query(\"Hello my World\")\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "27ca1901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length : 384\n"
     ]
    }
   ],
   "source": [
    "print(f\"length : {len(vector)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e855d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4acd3672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6a682dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY=os.getenv(\"GOOGLE_API_KEY\")\n",
    "PINECONE_API_KEY=os.getenv(\"PINECONE_API_KEY\")\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4c1393c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "pinecone_api_key = PINECONE_API_KEY\n",
    "\n",
    "pc = Pinecone(api_key=pinecone_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4996657c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pinecone.pinecone.Pinecone at 0x16b837929b0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0eb5d2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import ServerlessSpec\n",
    "index_name = \"medical-chatbot\"\n",
    "\n",
    "if not pc.has_index(index_name):\n",
    "    pc.create_index(name=index_name, dimension=384,metric=\"cosine\",spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"))\n",
    "\n",
    "index = pc.Index(index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "94aff60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "docsearch = PineconeVectorStore.from_documents(documents=text_chunk, embedding=embedding, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ed4fa23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "docsearch = PineconeVectorStore.from_existing_index( embedding=embedding, index_name=index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0838db8b",
   "metadata": {},
   "source": [
    "# Add more data to the existing pinecone index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf1bb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "dswith = Document(page_content=\"Adicodehub Youtube channel\", metadata={\"source\": \"youtube\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "60e54981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['997a5bb0-0ae8-42e3-ad85-dff341787abb']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docsearch.add_documents(documents=[dswith])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3799b6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = docsearch.as_retriever(search_type = \"similarity\", search_kwargs = {\"k\":3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c5c06d8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='fe2bd52f-541c-49e4-8f6c-327478591e9a', metadata={'source': 'data\\\\datascience.pdf'}, page_content='C u r a t e d  b y\\nWhat is the role of a data scientist in an \\norganisation?\\nExplain the difference between supervised and \\nunsupervised learning.\\r\\nWhat is cross-validation, and why is it important?\\r\\nA data scientist is responsible for collecting, analysing, \\nand interpreting complex data to help organisations \\nmake informed decisions.\\nSupervised learning uses labelled data for training, \\nwhile unsupervised learning\\rworks with unlabeled \\ndata to find hidden patterns or relationships.'),\n",
       " Document(id='6db9b974-c3f6-455d-9c38-ee4d555c7179', metadata={'source': 'data\\\\datascience.pdf'}, page_content='with These\\n100 Ques tions\\nData Scientist \\nInterview\\nAce your Next'),\n",
       " Document(id='a5600358-801b-4c89-8244-747b452bf79e', metadata={'source': 'data\\\\datascience.pdf'}, page_content='preventing certain features\\rfrom dominating the \\nlearning process.\\nAssociation rules are used to discover interesting \\nrelationships between variables\\rin large datasets. An \\nexample is market basket analysis used to identify \\nproducts frequently\\rpurchased together.\\nThe logistic function is used to model the probability of \\na binary outcome. It maps\\rany real-valued number to \\na value between 0 and 1, making it suitable for binary\\r\\nclassification tasks.\\nQ.96\\nQ.97\\nQ.98')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs = retriever.invoke(\"What is the role of a data scientist?\")\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0253e438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "chatmodel = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "802fc521",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from  langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9ce69707",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_promt = (\n",
    "    \"You are an assistant for question-answering tasks.\"\n",
    "    \"Use the following pieces Of retrieved context to answer \"\n",
    "    \"the question. If you I don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",system_promt),\n",
    "        (\"human\",\"{input}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "02923f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_chain = create_stuff_documents_chain(chatmodel,prompt)\n",
    "rag_chain = create_retrieval_chain(retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c34a4ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A data scientist is responsible for collecting, analyzing, and interpreting complex data. Their primary role is to help organizations make informed decisions based on these insights.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\":\"What is the role of a data scientist?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb669410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data preprocessing involves preparing and cleaning data before it is fed into a machine learning model. It includes data cleaning, handling missing values, data transformation, normalisation, and standardisation. These steps are crucial to prepare the data for analysis and modelling.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke({\"input\":\"Data preprocessing process?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "71c82e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supervised learning uses labelled data for training. It involves an algorithm learning from input-output pairs to predict outcomes for new, unseen data.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke(({\"input\":\"what is supervised learning?\"}))\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973fd01f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
